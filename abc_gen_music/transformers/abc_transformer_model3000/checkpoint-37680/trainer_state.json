{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 37680,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03980891719745223,
      "grad_norm": 9.300448417663574,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 11.9225,
      "step": 300
    },
    {
      "epoch": 0.07961783439490445,
      "grad_norm": 5.868102073669434,
      "learning_rate": 4.9866863905325444e-05,
      "loss": 8.9169,
      "step": 600
    },
    {
      "epoch": 0.11942675159235669,
      "grad_norm": 5.327516555786133,
      "learning_rate": 4.946342119419043e-05,
      "loss": 8.1728,
      "step": 900
    },
    {
      "epoch": 0.1592356687898089,
      "grad_norm": 5.485913276672363,
      "learning_rate": 4.905997848305541e-05,
      "loss": 7.4352,
      "step": 1200
    },
    {
      "epoch": 0.19904458598726116,
      "grad_norm": 6.7301506996154785,
      "learning_rate": 4.865653577192039e-05,
      "loss": 6.8542,
      "step": 1500
    },
    {
      "epoch": 0.23885350318471338,
      "grad_norm": 6.313752174377441,
      "learning_rate": 4.825309306078537e-05,
      "loss": 6.4181,
      "step": 1800
    },
    {
      "epoch": 0.2786624203821656,
      "grad_norm": 5.187729358673096,
      "learning_rate": 4.7849650349650356e-05,
      "loss": 6.084,
      "step": 2100
    },
    {
      "epoch": 0.3184713375796178,
      "grad_norm": 5.70046329498291,
      "learning_rate": 4.7446207638515336e-05,
      "loss": 5.7525,
      "step": 2400
    },
    {
      "epoch": 0.35828025477707004,
      "grad_norm": 5.431107044219971,
      "learning_rate": 4.704276492738031e-05,
      "loss": 5.4483,
      "step": 2700
    },
    {
      "epoch": 0.3980891719745223,
      "grad_norm": 5.77890682220459,
      "learning_rate": 4.6639322216245295e-05,
      "loss": 5.2034,
      "step": 3000
    },
    {
      "epoch": 0.43789808917197454,
      "grad_norm": 5.1943359375,
      "learning_rate": 4.6235879505110275e-05,
      "loss": 4.967,
      "step": 3300
    },
    {
      "epoch": 0.47770700636942676,
      "grad_norm": 5.916321754455566,
      "learning_rate": 4.583243679397526e-05,
      "loss": 4.8002,
      "step": 3600
    },
    {
      "epoch": 0.517515923566879,
      "grad_norm": 5.898303031921387,
      "learning_rate": 4.5428994082840234e-05,
      "loss": 4.5809,
      "step": 3900
    },
    {
      "epoch": 0.5573248407643312,
      "grad_norm": 5.576845645904541,
      "learning_rate": 4.502555137170522e-05,
      "loss": 4.4002,
      "step": 4200
    },
    {
      "epoch": 0.5971337579617835,
      "grad_norm": 5.4527482986450195,
      "learning_rate": 4.46221086605702e-05,
      "loss": 4.2554,
      "step": 4500
    },
    {
      "epoch": 0.6369426751592356,
      "grad_norm": 5.844244003295898,
      "learning_rate": 4.421866594943519e-05,
      "loss": 4.0672,
      "step": 4800
    },
    {
      "epoch": 0.6767515923566879,
      "grad_norm": 6.031475067138672,
      "learning_rate": 4.381522323830016e-05,
      "loss": 3.9515,
      "step": 5100
    },
    {
      "epoch": 0.7165605095541401,
      "grad_norm": 6.442119598388672,
      "learning_rate": 4.3411780527165146e-05,
      "loss": 3.8149,
      "step": 5400
    },
    {
      "epoch": 0.7563694267515924,
      "grad_norm": 7.001306056976318,
      "learning_rate": 4.3008337816030126e-05,
      "loss": 3.674,
      "step": 5700
    },
    {
      "epoch": 0.7961783439490446,
      "grad_norm": 6.6598687171936035,
      "learning_rate": 4.260489510489511e-05,
      "loss": 3.5502,
      "step": 6000
    },
    {
      "epoch": 0.8359872611464968,
      "grad_norm": 7.650022506713867,
      "learning_rate": 4.2201452393760085e-05,
      "loss": 3.3882,
      "step": 6300
    },
    {
      "epoch": 0.8757961783439491,
      "grad_norm": 6.944546699523926,
      "learning_rate": 4.1798009682625065e-05,
      "loss": 3.2862,
      "step": 6600
    },
    {
      "epoch": 0.9156050955414012,
      "grad_norm": 7.085702896118164,
      "learning_rate": 4.139456697149005e-05,
      "loss": 3.1461,
      "step": 6900
    },
    {
      "epoch": 0.9554140127388535,
      "grad_norm": 6.605510711669922,
      "learning_rate": 4.099112426035503e-05,
      "loss": 3.0417,
      "step": 7200
    },
    {
      "epoch": 0.9952229299363057,
      "grad_norm": 7.022542953491211,
      "learning_rate": 4.058768154922001e-05,
      "loss": 2.9448,
      "step": 7500
    },
    {
      "epoch": 1.035031847133758,
      "grad_norm": 7.43017053604126,
      "learning_rate": 4.018558364712211e-05,
      "loss": 2.7386,
      "step": 7800
    },
    {
      "epoch": 1.0748407643312101,
      "grad_norm": 7.240838050842285,
      "learning_rate": 3.9782140935987096e-05,
      "loss": 2.6319,
      "step": 8100
    },
    {
      "epoch": 1.1146496815286624,
      "grad_norm": 7.3754401206970215,
      "learning_rate": 3.9378698224852076e-05,
      "loss": 2.5632,
      "step": 8400
    },
    {
      "epoch": 1.1544585987261147,
      "grad_norm": 7.501034736633301,
      "learning_rate": 3.897525551371705e-05,
      "loss": 2.4905,
      "step": 8700
    },
    {
      "epoch": 1.194267515923567,
      "grad_norm": 7.305665016174316,
      "learning_rate": 3.8571812802582036e-05,
      "loss": 2.3993,
      "step": 9000
    },
    {
      "epoch": 1.234076433121019,
      "grad_norm": 8.080227851867676,
      "learning_rate": 3.8168370091447015e-05,
      "loss": 2.3241,
      "step": 9300
    },
    {
      "epoch": 1.2738853503184713,
      "grad_norm": 7.195990562438965,
      "learning_rate": 3.7764927380312e-05,
      "loss": 2.2361,
      "step": 9600
    },
    {
      "epoch": 1.3136942675159236,
      "grad_norm": 7.593464374542236,
      "learning_rate": 3.7361484669176975e-05,
      "loss": 2.1759,
      "step": 9900
    },
    {
      "epoch": 1.3535031847133758,
      "grad_norm": 7.262654781341553,
      "learning_rate": 3.695804195804196e-05,
      "loss": 2.1024,
      "step": 10200
    },
    {
      "epoch": 1.393312101910828,
      "grad_norm": 7.7392191886901855,
      "learning_rate": 3.655459924690694e-05,
      "loss": 2.0254,
      "step": 10500
    },
    {
      "epoch": 1.4331210191082802,
      "grad_norm": 7.48189115524292,
      "learning_rate": 3.615115653577192e-05,
      "loss": 1.9876,
      "step": 10800
    },
    {
      "epoch": 1.4729299363057324,
      "grad_norm": 7.222124099731445,
      "learning_rate": 3.57477138246369e-05,
      "loss": 1.915,
      "step": 11100
    },
    {
      "epoch": 1.5127388535031847,
      "grad_norm": 7.796530723571777,
      "learning_rate": 3.534427111350189e-05,
      "loss": 1.8391,
      "step": 11400
    },
    {
      "epoch": 1.552547770700637,
      "grad_norm": 7.744855880737305,
      "learning_rate": 3.4942173211403986e-05,
      "loss": 1.7926,
      "step": 11700
    },
    {
      "epoch": 1.5923566878980893,
      "grad_norm": 7.936404705047607,
      "learning_rate": 3.4538730500268965e-05,
      "loss": 1.7605,
      "step": 12000
    },
    {
      "epoch": 1.6321656050955413,
      "grad_norm": 7.751623630523682,
      "learning_rate": 3.4135287789133945e-05,
      "loss": 1.6799,
      "step": 12300
    },
    {
      "epoch": 1.6719745222929936,
      "grad_norm": 7.631894588470459,
      "learning_rate": 3.3731845077998925e-05,
      "loss": 1.6312,
      "step": 12600
    },
    {
      "epoch": 1.7117834394904459,
      "grad_norm": 7.831223011016846,
      "learning_rate": 3.332840236686391e-05,
      "loss": 1.5984,
      "step": 12900
    },
    {
      "epoch": 1.7515923566878981,
      "grad_norm": 7.498437404632568,
      "learning_rate": 3.2924959655728884e-05,
      "loss": 1.5322,
      "step": 13200
    },
    {
      "epoch": 1.7914012738853504,
      "grad_norm": 6.834988594055176,
      "learning_rate": 3.252151694459387e-05,
      "loss": 1.5012,
      "step": 13500
    },
    {
      "epoch": 1.8312101910828025,
      "grad_norm": 7.242244720458984,
      "learning_rate": 3.211941904249597e-05,
      "loss": 1.4527,
      "step": 13800
    },
    {
      "epoch": 1.8710191082802548,
      "grad_norm": 7.146771430969238,
      "learning_rate": 3.171597633136095e-05,
      "loss": 1.4162,
      "step": 14100
    },
    {
      "epoch": 1.910828025477707,
      "grad_norm": 7.43782377243042,
      "learning_rate": 3.131253362022593e-05,
      "loss": 1.3672,
      "step": 14400
    },
    {
      "epoch": 1.950636942675159,
      "grad_norm": 7.684304714202881,
      "learning_rate": 3.090909090909091e-05,
      "loss": 1.3436,
      "step": 14700
    },
    {
      "epoch": 1.9904458598726116,
      "grad_norm": 8.305597305297852,
      "learning_rate": 3.0505648197955895e-05,
      "loss": 1.2959,
      "step": 15000
    },
    {
      "epoch": 2.0302547770700636,
      "grad_norm": 6.950399875640869,
      "learning_rate": 3.010355029585799e-05,
      "loss": 1.1735,
      "step": 15300
    },
    {
      "epoch": 2.070063694267516,
      "grad_norm": 7.488353729248047,
      "learning_rate": 2.970010758472297e-05,
      "loss": 1.1524,
      "step": 15600
    },
    {
      "epoch": 2.109872611464968,
      "grad_norm": 7.229193687438965,
      "learning_rate": 2.9296664873587954e-05,
      "loss": 1.1067,
      "step": 15900
    },
    {
      "epoch": 2.1496815286624202,
      "grad_norm": 7.11448860168457,
      "learning_rate": 2.8893222162452933e-05,
      "loss": 1.1007,
      "step": 16200
    },
    {
      "epoch": 2.1894904458598727,
      "grad_norm": 7.718127727508545,
      "learning_rate": 2.8489779451317917e-05,
      "loss": 1.0781,
      "step": 16500
    },
    {
      "epoch": 2.229299363057325,
      "grad_norm": 8.049558639526367,
      "learning_rate": 2.8086336740182896e-05,
      "loss": 1.0377,
      "step": 16800
    },
    {
      "epoch": 2.269108280254777,
      "grad_norm": 7.313186168670654,
      "learning_rate": 2.768289402904788e-05,
      "loss": 1.0235,
      "step": 17100
    },
    {
      "epoch": 2.3089171974522293,
      "grad_norm": 6.280001163482666,
      "learning_rate": 2.727945131791286e-05,
      "loss": 1.0083,
      "step": 17400
    },
    {
      "epoch": 2.3487261146496814,
      "grad_norm": 6.349170684814453,
      "learning_rate": 2.6876008606777842e-05,
      "loss": 0.9694,
      "step": 17700
    },
    {
      "epoch": 2.388535031847134,
      "grad_norm": 7.314239978790283,
      "learning_rate": 2.6472565895642822e-05,
      "loss": 0.9566,
      "step": 18000
    },
    {
      "epoch": 2.428343949044586,
      "grad_norm": 7.246012210845947,
      "learning_rate": 2.6069123184507798e-05,
      "loss": 0.9299,
      "step": 18300
    },
    {
      "epoch": 2.468152866242038,
      "grad_norm": 7.379730224609375,
      "learning_rate": 2.566568047337278e-05,
      "loss": 0.911,
      "step": 18600
    },
    {
      "epoch": 2.5079617834394905,
      "grad_norm": 7.511988639831543,
      "learning_rate": 2.526223776223776e-05,
      "loss": 0.881,
      "step": 18900
    },
    {
      "epoch": 2.5477707006369426,
      "grad_norm": 7.7402849197387695,
      "learning_rate": 2.486013986013986e-05,
      "loss": 0.8543,
      "step": 19200
    },
    {
      "epoch": 2.587579617834395,
      "grad_norm": 7.400706768035889,
      "learning_rate": 2.4456697149004843e-05,
      "loss": 0.8478,
      "step": 19500
    },
    {
      "epoch": 2.627388535031847,
      "grad_norm": 7.2565531730651855,
      "learning_rate": 2.4053254437869823e-05,
      "loss": 0.8196,
      "step": 19800
    },
    {
      "epoch": 2.667197452229299,
      "grad_norm": 6.682875156402588,
      "learning_rate": 2.3649811726734806e-05,
      "loss": 0.7964,
      "step": 20100
    },
    {
      "epoch": 2.7070063694267517,
      "grad_norm": 8.117366790771484,
      "learning_rate": 2.3246369015599786e-05,
      "loss": 0.7881,
      "step": 20400
    },
    {
      "epoch": 2.7468152866242037,
      "grad_norm": 7.325249195098877,
      "learning_rate": 2.284292630446477e-05,
      "loss": 0.7689,
      "step": 20700
    },
    {
      "epoch": 2.786624203821656,
      "grad_norm": 6.874610424041748,
      "learning_rate": 2.243948359332975e-05,
      "loss": 0.743,
      "step": 21000
    },
    {
      "epoch": 2.8264331210191083,
      "grad_norm": 6.71039342880249,
      "learning_rate": 2.2037385691231847e-05,
      "loss": 0.7196,
      "step": 21300
    },
    {
      "epoch": 2.8662420382165603,
      "grad_norm": 7.329055309295654,
      "learning_rate": 2.1633942980096827e-05,
      "loss": 0.7089,
      "step": 21600
    },
    {
      "epoch": 2.906050955414013,
      "grad_norm": 6.7981672286987305,
      "learning_rate": 2.123050026896181e-05,
      "loss": 0.6948,
      "step": 21900
    },
    {
      "epoch": 2.945859872611465,
      "grad_norm": 7.122475624084473,
      "learning_rate": 2.082705755782679e-05,
      "loss": 0.6814,
      "step": 22200
    },
    {
      "epoch": 2.9856687898089174,
      "grad_norm": 7.385252475738525,
      "learning_rate": 2.0423614846691773e-05,
      "loss": 0.6634,
      "step": 22500
    },
    {
      "epoch": 3.0254777070063694,
      "grad_norm": 6.915943622589111,
      "learning_rate": 2.002151694459387e-05,
      "loss": 0.6094,
      "step": 22800
    },
    {
      "epoch": 3.0652866242038215,
      "grad_norm": 6.756034851074219,
      "learning_rate": 1.9618074233458848e-05,
      "loss": 0.571,
      "step": 23100
    },
    {
      "epoch": 3.105095541401274,
      "grad_norm": 5.821683883666992,
      "learning_rate": 1.921463152232383e-05,
      "loss": 0.5715,
      "step": 23400
    },
    {
      "epoch": 3.144904458598726,
      "grad_norm": 6.536971569061279,
      "learning_rate": 1.881118881118881e-05,
      "loss": 0.5569,
      "step": 23700
    },
    {
      "epoch": 3.1847133757961785,
      "grad_norm": 6.345478534698486,
      "learning_rate": 1.8407746100053794e-05,
      "loss": 0.5461,
      "step": 24000
    },
    {
      "epoch": 3.2245222929936306,
      "grad_norm": 6.136021137237549,
      "learning_rate": 1.8004303388918774e-05,
      "loss": 0.5377,
      "step": 24300
    },
    {
      "epoch": 3.2643312101910826,
      "grad_norm": 6.322657108306885,
      "learning_rate": 1.7600860677783757e-05,
      "loss": 0.5245,
      "step": 24600
    },
    {
      "epoch": 3.304140127388535,
      "grad_norm": 6.986168384552002,
      "learning_rate": 1.7197417966648737e-05,
      "loss": 0.5137,
      "step": 24900
    },
    {
      "epoch": 3.343949044585987,
      "grad_norm": 6.482163906097412,
      "learning_rate": 1.679397525551372e-05,
      "loss": 0.4982,
      "step": 25200
    },
    {
      "epoch": 3.3837579617834397,
      "grad_norm": 6.322555065155029,
      "learning_rate": 1.63905325443787e-05,
      "loss": 0.4955,
      "step": 25500
    },
    {
      "epoch": 3.4235668789808917,
      "grad_norm": 6.379969120025635,
      "learning_rate": 1.598708983324368e-05,
      "loss": 0.482,
      "step": 25800
    },
    {
      "epoch": 3.463375796178344,
      "grad_norm": 6.472056865692139,
      "learning_rate": 1.558364712210866e-05,
      "loss": 0.4758,
      "step": 26100
    },
    {
      "epoch": 3.5031847133757963,
      "grad_norm": 5.972443103790283,
      "learning_rate": 1.5180204410973642e-05,
      "loss": 0.464,
      "step": 26400
    },
    {
      "epoch": 3.5429936305732483,
      "grad_norm": 6.117671489715576,
      "learning_rate": 1.4778106508875741e-05,
      "loss": 0.4566,
      "step": 26700
    },
    {
      "epoch": 3.582802547770701,
      "grad_norm": 7.071564674377441,
      "learning_rate": 1.4374663797740722e-05,
      "loss": 0.4476,
      "step": 27000
    },
    {
      "epoch": 3.622611464968153,
      "grad_norm": 6.864388465881348,
      "learning_rate": 1.3971221086605702e-05,
      "loss": 0.4361,
      "step": 27300
    },
    {
      "epoch": 3.662420382165605,
      "grad_norm": 5.802245140075684,
      "learning_rate": 1.3567778375470683e-05,
      "loss": 0.4303,
      "step": 27600
    },
    {
      "epoch": 3.7022292993630574,
      "grad_norm": 5.921624183654785,
      "learning_rate": 1.3164335664335665e-05,
      "loss": 0.421,
      "step": 27900
    },
    {
      "epoch": 3.7420382165605095,
      "grad_norm": 6.1706037521362305,
      "learning_rate": 1.2760892953200646e-05,
      "loss": 0.4125,
      "step": 28200
    },
    {
      "epoch": 3.781847133757962,
      "grad_norm": 5.550814628601074,
      "learning_rate": 1.2357450242065628e-05,
      "loss": 0.3971,
      "step": 28500
    },
    {
      "epoch": 3.821656050955414,
      "grad_norm": 6.101742744445801,
      "learning_rate": 1.1954007530930607e-05,
      "loss": 0.3894,
      "step": 28800
    },
    {
      "epoch": 3.861464968152866,
      "grad_norm": 6.294634819030762,
      "learning_rate": 1.1551909628832706e-05,
      "loss": 0.3887,
      "step": 29100
    },
    {
      "epoch": 3.9012738853503186,
      "grad_norm": 6.397733211517334,
      "learning_rate": 1.1148466917697688e-05,
      "loss": 0.3818,
      "step": 29400
    },
    {
      "epoch": 3.9410828025477707,
      "grad_norm": 5.743150234222412,
      "learning_rate": 1.0745024206562669e-05,
      "loss": 0.378,
      "step": 29700
    },
    {
      "epoch": 3.980891719745223,
      "grad_norm": 5.536024570465088,
      "learning_rate": 1.0341581495427649e-05,
      "loss": 0.3604,
      "step": 30000
    },
    {
      "epoch": 4.020700636942675,
      "grad_norm": 6.028337478637695,
      "learning_rate": 9.93813878429263e-06,
      "loss": 0.3356,
      "step": 30300
    },
    {
      "epoch": 4.060509554140127,
      "grad_norm": 5.515843868255615,
      "learning_rate": 9.534696073157612e-06,
      "loss": 0.3132,
      "step": 30600
    },
    {
      "epoch": 4.100318471337579,
      "grad_norm": 5.8571391105651855,
      "learning_rate": 9.131253362022593e-06,
      "loss": 0.3133,
      "step": 30900
    },
    {
      "epoch": 4.140127388535032,
      "grad_norm": 4.5769267082214355,
      "learning_rate": 8.727810650887574e-06,
      "loss": 0.3051,
      "step": 31200
    },
    {
      "epoch": 4.179936305732484,
      "grad_norm": 5.7004499435424805,
      "learning_rate": 8.324367939752556e-06,
      "loss": 0.302,
      "step": 31500
    },
    {
      "epoch": 4.219745222929936,
      "grad_norm": 5.737386226654053,
      "learning_rate": 7.920925228617537e-06,
      "loss": 0.3011,
      "step": 31800
    },
    {
      "epoch": 4.259554140127388,
      "grad_norm": 5.39339017868042,
      "learning_rate": 7.517482517482517e-06,
      "loss": 0.2945,
      "step": 32100
    },
    {
      "epoch": 4.2993630573248405,
      "grad_norm": 5.789310932159424,
      "learning_rate": 7.114039806347498e-06,
      "loss": 0.2891,
      "step": 32400
    },
    {
      "epoch": 4.3391719745222925,
      "grad_norm": 5.463088035583496,
      "learning_rate": 6.71059709521248e-06,
      "loss": 0.2862,
      "step": 32700
    },
    {
      "epoch": 4.3789808917197455,
      "grad_norm": 5.608162879943848,
      "learning_rate": 6.308499193114578e-06,
      "loss": 0.2775,
      "step": 33000
    },
    {
      "epoch": 4.4187898089171975,
      "grad_norm": 5.797525882720947,
      "learning_rate": 5.906401291016676e-06,
      "loss": 0.2742,
      "step": 33300
    },
    {
      "epoch": 4.45859872611465,
      "grad_norm": 4.755066871643066,
      "learning_rate": 5.502958579881657e-06,
      "loss": 0.2705,
      "step": 33600
    },
    {
      "epoch": 4.498407643312102,
      "grad_norm": 5.034250259399414,
      "learning_rate": 5.099515868746638e-06,
      "loss": 0.2699,
      "step": 33900
    },
    {
      "epoch": 4.538216560509554,
      "grad_norm": 5.099295139312744,
      "learning_rate": 4.696073157611619e-06,
      "loss": 0.2655,
      "step": 34200
    },
    {
      "epoch": 4.578025477707007,
      "grad_norm": 4.532005310058594,
      "learning_rate": 4.292630446476601e-06,
      "loss": 0.2593,
      "step": 34500
    },
    {
      "epoch": 4.617834394904459,
      "grad_norm": 5.232638359069824,
      "learning_rate": 3.889187735341582e-06,
      "loss": 0.2517,
      "step": 34800
    },
    {
      "epoch": 4.657643312101911,
      "grad_norm": 5.2685546875,
      "learning_rate": 3.485745024206563e-06,
      "loss": 0.2508,
      "step": 35100
    },
    {
      "epoch": 4.697452229299363,
      "grad_norm": 5.247182369232178,
      "learning_rate": 3.082302313071544e-06,
      "loss": 0.2461,
      "step": 35400
    },
    {
      "epoch": 4.737261146496815,
      "grad_norm": 4.917637348175049,
      "learning_rate": 2.678859601936525e-06,
      "loss": 0.2451,
      "step": 35700
    },
    {
      "epoch": 4.777070063694268,
      "grad_norm": 4.94786262512207,
      "learning_rate": 2.275416890801506e-06,
      "loss": 0.2394,
      "step": 36000
    },
    {
      "epoch": 4.81687898089172,
      "grad_norm": 4.651788711547852,
      "learning_rate": 1.8719741796664875e-06,
      "loss": 0.2391,
      "step": 36300
    },
    {
      "epoch": 4.856687898089172,
      "grad_norm": 5.2481889724731445,
      "learning_rate": 1.4685314685314685e-06,
      "loss": 0.2403,
      "step": 36600
    },
    {
      "epoch": 4.896496815286624,
      "grad_norm": 4.916642189025879,
      "learning_rate": 1.0650887573964497e-06,
      "loss": 0.2304,
      "step": 36900
    },
    {
      "epoch": 4.936305732484076,
      "grad_norm": 5.223803997039795,
      "learning_rate": 6.629908552985477e-07,
      "loss": 0.2336,
      "step": 37200
    },
    {
      "epoch": 4.976114649681529,
      "grad_norm": 5.018339157104492,
      "learning_rate": 2.595481441635288e-07,
      "loss": 0.2277,
      "step": 37500
    }
  ],
  "logging_steps": 300,
  "max_steps": 37680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.6113561338502656e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
