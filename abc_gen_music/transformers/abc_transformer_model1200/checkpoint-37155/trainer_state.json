{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.999932718831999,
  "eval_steps": 500,
  "global_step": 37155,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0403687008006459,
      "grad_norm": 8.109127044677734,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 10.4631,
      "step": 300
    },
    {
      "epoch": 0.0807374016012918,
      "grad_norm": 7.585738658905029,
      "learning_rate": 4.9864957031782845e-05,
      "loss": 7.8818,
      "step": 600
    },
    {
      "epoch": 0.1211061024019377,
      "grad_norm": 6.29105281829834,
      "learning_rate": 4.945573591597327e-05,
      "loss": 6.751,
      "step": 900
    },
    {
      "epoch": 0.1614748032025836,
      "grad_norm": 5.462175369262695,
      "learning_rate": 4.904651480016369e-05,
      "loss": 6.0352,
      "step": 1200
    },
    {
      "epoch": 0.2018435040032295,
      "grad_norm": 6.325969696044922,
      "learning_rate": 4.8637293684354114e-05,
      "loss": 5.5215,
      "step": 1500
    },
    {
      "epoch": 0.2422122048038754,
      "grad_norm": 5.4798407554626465,
      "learning_rate": 4.8228072568544543e-05,
      "loss": 5.1364,
      "step": 1800
    },
    {
      "epoch": 0.2825809056045213,
      "grad_norm": 5.878586292266846,
      "learning_rate": 4.7818851452734966e-05,
      "loss": 4.8252,
      "step": 2100
    },
    {
      "epoch": 0.3229496064051672,
      "grad_norm": 5.079990863800049,
      "learning_rate": 4.740963033692539e-05,
      "loss": 4.5643,
      "step": 2400
    },
    {
      "epoch": 0.3633183072058131,
      "grad_norm": 5.011752605438232,
      "learning_rate": 4.700040922111581e-05,
      "loss": 4.3464,
      "step": 2700
    },
    {
      "epoch": 0.403687008006459,
      "grad_norm": 5.090227127075195,
      "learning_rate": 4.6591188105306235e-05,
      "loss": 4.1774,
      "step": 3000
    },
    {
      "epoch": 0.4440557088071049,
      "grad_norm": 5.518662929534912,
      "learning_rate": 4.618196698949666e-05,
      "loss": 4.0,
      "step": 3300
    },
    {
      "epoch": 0.4844244096077508,
      "grad_norm": 5.17441463470459,
      "learning_rate": 4.577274587368708e-05,
      "loss": 3.8404,
      "step": 3600
    },
    {
      "epoch": 0.5247931104083967,
      "grad_norm": 5.440970420837402,
      "learning_rate": 4.5363524757877504e-05,
      "loss": 3.6989,
      "step": 3900
    },
    {
      "epoch": 0.5651618112090426,
      "grad_norm": 5.273592472076416,
      "learning_rate": 4.4954303642067934e-05,
      "loss": 3.5713,
      "step": 4200
    },
    {
      "epoch": 0.6055305120096884,
      "grad_norm": 5.173779010772705,
      "learning_rate": 4.454508252625836e-05,
      "loss": 3.4548,
      "step": 4500
    },
    {
      "epoch": 0.6458992128103344,
      "grad_norm": 5.289348125457764,
      "learning_rate": 4.413586141044878e-05,
      "loss": 3.3425,
      "step": 4800
    },
    {
      "epoch": 0.6862679136109803,
      "grad_norm": 5.626559257507324,
      "learning_rate": 4.37266402946392e-05,
      "loss": 3.2003,
      "step": 5100
    },
    {
      "epoch": 0.7266366144116262,
      "grad_norm": 5.690260887145996,
      "learning_rate": 4.331741917882963e-05,
      "loss": 3.0841,
      "step": 5400
    },
    {
      "epoch": 0.7670053152122721,
      "grad_norm": 6.47828483581543,
      "learning_rate": 4.2908198063020056e-05,
      "loss": 3.009,
      "step": 5700
    },
    {
      "epoch": 0.807374016012918,
      "grad_norm": 5.974367141723633,
      "learning_rate": 4.249897694721048e-05,
      "loss": 2.854,
      "step": 6000
    },
    {
      "epoch": 0.8477427168135638,
      "grad_norm": 5.654534339904785,
      "learning_rate": 4.20897558314009e-05,
      "loss": 2.7381,
      "step": 6300
    },
    {
      "epoch": 0.8881114176142098,
      "grad_norm": 6.863559722900391,
      "learning_rate": 4.1680534715591325e-05,
      "loss": 2.6596,
      "step": 6600
    },
    {
      "epoch": 0.9284801184148557,
      "grad_norm": 6.454719066619873,
      "learning_rate": 4.1271313599781754e-05,
      "loss": 2.5436,
      "step": 6900
    },
    {
      "epoch": 0.9688488192155016,
      "grad_norm": 6.550110816955566,
      "learning_rate": 4.086209248397218e-05,
      "loss": 2.4576,
      "step": 7200
    },
    {
      "epoch": 1.0091502388481464,
      "grad_norm": 6.139616012573242,
      "learning_rate": 4.04528713681626e-05,
      "loss": 2.3484,
      "step": 7500
    },
    {
      "epoch": 1.0495189396487923,
      "grad_norm": 6.5544891357421875,
      "learning_rate": 4.004365025235302e-05,
      "loss": 2.2182,
      "step": 7800
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 6.561300277709961,
      "learning_rate": 3.9635793206929477e-05,
      "loss": 2.1485,
      "step": 8100
    },
    {
      "epoch": 1.1302563412500841,
      "grad_norm": 6.004348278045654,
      "learning_rate": 3.9226572091119906e-05,
      "loss": 2.0726,
      "step": 8400
    },
    {
      "epoch": 1.17062504205073,
      "grad_norm": 6.595462322235107,
      "learning_rate": 3.881735097531033e-05,
      "loss": 1.982,
      "step": 8700
    },
    {
      "epoch": 1.210993742851376,
      "grad_norm": 6.386993408203125,
      "learning_rate": 3.840812985950075e-05,
      "loss": 1.9218,
      "step": 9000
    },
    {
      "epoch": 1.2513624436520219,
      "grad_norm": 6.809554100036621,
      "learning_rate": 3.7998908743691175e-05,
      "loss": 1.8679,
      "step": 9300
    },
    {
      "epoch": 1.2917311444526676,
      "grad_norm": 6.635235786437988,
      "learning_rate": 3.75896876278816e-05,
      "loss": 1.7949,
      "step": 9600
    },
    {
      "epoch": 1.3320998452533135,
      "grad_norm": 6.304062843322754,
      "learning_rate": 3.718046651207203e-05,
      "loss": 1.7386,
      "step": 9900
    },
    {
      "epoch": 1.3724685460539594,
      "grad_norm": 7.16951322555542,
      "learning_rate": 3.677260946664848e-05,
      "loss": 1.69,
      "step": 10200
    },
    {
      "epoch": 1.4128372468546053,
      "grad_norm": 6.843472003936768,
      "learning_rate": 3.6363388350838904e-05,
      "loss": 1.641,
      "step": 10500
    },
    {
      "epoch": 1.4532059476552512,
      "grad_norm": 6.335492134094238,
      "learning_rate": 3.595416723502933e-05,
      "loss": 1.5866,
      "step": 10800
    },
    {
      "epoch": 1.4935746484558972,
      "grad_norm": 6.943267822265625,
      "learning_rate": 3.554494611921975e-05,
      "loss": 1.5416,
      "step": 11100
    },
    {
      "epoch": 1.533943349256543,
      "grad_norm": 6.451644420623779,
      "learning_rate": 3.513572500341018e-05,
      "loss": 1.4781,
      "step": 11400
    },
    {
      "epoch": 1.574312050057189,
      "grad_norm": 6.372734546661377,
      "learning_rate": 3.47265038876006e-05,
      "loss": 1.4518,
      "step": 11700
    },
    {
      "epoch": 1.614680750857835,
      "grad_norm": 7.0216803550720215,
      "learning_rate": 3.4317282771791026e-05,
      "loss": 1.4049,
      "step": 12000
    },
    {
      "epoch": 1.6550494516584808,
      "grad_norm": 7.20265007019043,
      "learning_rate": 3.3909425726367486e-05,
      "loss": 1.363,
      "step": 12300
    },
    {
      "epoch": 1.6954181524591267,
      "grad_norm": 7.166936874389648,
      "learning_rate": 3.350020461055791e-05,
      "loss": 1.331,
      "step": 12600
    },
    {
      "epoch": 1.7357868532597727,
      "grad_norm": 6.8386759757995605,
      "learning_rate": 3.309098349474833e-05,
      "loss": 1.2848,
      "step": 12900
    },
    {
      "epoch": 1.7761555540604186,
      "grad_norm": 7.069629669189453,
      "learning_rate": 3.2681762378938755e-05,
      "loss": 1.2445,
      "step": 13200
    },
    {
      "epoch": 1.8165242548610645,
      "grad_norm": 6.105255126953125,
      "learning_rate": 3.227254126312918e-05,
      "loss": 1.2156,
      "step": 13500
    },
    {
      "epoch": 1.8568929556617104,
      "grad_norm": 6.457767486572266,
      "learning_rate": 3.18633201473196e-05,
      "loss": 1.1773,
      "step": 13800
    },
    {
      "epoch": 1.8972616564623563,
      "grad_norm": 6.478869915008545,
      "learning_rate": 3.145546310189606e-05,
      "loss": 1.1588,
      "step": 14100
    },
    {
      "epoch": 1.937630357263002,
      "grad_norm": 6.808119773864746,
      "learning_rate": 3.1046241986086484e-05,
      "loss": 1.1149,
      "step": 14400
    },
    {
      "epoch": 1.977999058063648,
      "grad_norm": 6.47138786315918,
      "learning_rate": 3.0637020870276907e-05,
      "loss": 1.0712,
      "step": 14700
    },
    {
      "epoch": 2.0183004776962927,
      "grad_norm": 5.983615398406982,
      "learning_rate": 3.0227799754467333e-05,
      "loss": 1.0214,
      "step": 15000
    },
    {
      "epoch": 2.0586691784969386,
      "grad_norm": 6.539683818817139,
      "learning_rate": 2.9818578638657756e-05,
      "loss": 0.9456,
      "step": 15300
    },
    {
      "epoch": 2.0990378792975846,
      "grad_norm": 6.151737213134766,
      "learning_rate": 2.940935752284818e-05,
      "loss": 0.9389,
      "step": 15600
    },
    {
      "epoch": 2.1394065800982305,
      "grad_norm": 6.674051761627197,
      "learning_rate": 2.9000136407038602e-05,
      "loss": 0.9225,
      "step": 15900
    },
    {
      "epoch": 2.1797752808988764,
      "grad_norm": 6.491724967956543,
      "learning_rate": 2.859227936161506e-05,
      "loss": 0.898,
      "step": 16200
    },
    {
      "epoch": 2.2201439816995223,
      "grad_norm": 6.663516521453857,
      "learning_rate": 2.818305824580548e-05,
      "loss": 0.8787,
      "step": 16500
    },
    {
      "epoch": 2.2605126825001682,
      "grad_norm": 6.529527187347412,
      "learning_rate": 2.777383712999591e-05,
      "loss": 0.8488,
      "step": 16800
    },
    {
      "epoch": 2.300881383300814,
      "grad_norm": 6.93217134475708,
      "learning_rate": 2.7364616014186334e-05,
      "loss": 0.8348,
      "step": 17100
    },
    {
      "epoch": 2.34125008410146,
      "grad_norm": 6.054935932159424,
      "learning_rate": 2.6955394898376757e-05,
      "loss": 0.8219,
      "step": 17400
    },
    {
      "epoch": 2.381618784902106,
      "grad_norm": 6.214072227478027,
      "learning_rate": 2.654617378256718e-05,
      "loss": 0.7864,
      "step": 17700
    },
    {
      "epoch": 2.421987485702752,
      "grad_norm": 6.584471702575684,
      "learning_rate": 2.6136952666757606e-05,
      "loss": 0.7741,
      "step": 18000
    },
    {
      "epoch": 2.462356186503398,
      "grad_norm": 6.197762489318848,
      "learning_rate": 2.572773155094803e-05,
      "loss": 0.7498,
      "step": 18300
    },
    {
      "epoch": 2.5027248873040437,
      "grad_norm": 6.181910037994385,
      "learning_rate": 2.531987450552449e-05,
      "loss": 0.7363,
      "step": 18600
    },
    {
      "epoch": 2.5430935881046897,
      "grad_norm": 5.725700855255127,
      "learning_rate": 2.4910653389714912e-05,
      "loss": 0.718,
      "step": 18900
    },
    {
      "epoch": 2.583462288905335,
      "grad_norm": 6.459646224975586,
      "learning_rate": 2.4501432273905335e-05,
      "loss": 0.6918,
      "step": 19200
    },
    {
      "epoch": 2.623830989705981,
      "grad_norm": 5.8625168800354,
      "learning_rate": 2.4092211158095758e-05,
      "loss": 0.677,
      "step": 19500
    },
    {
      "epoch": 2.6644688151786315,
      "grad_norm": 5.872984886169434,
      "learning_rate": 2.368299004228618e-05,
      "loss": 0.668,
      "step": 19800
    },
    {
      "epoch": 2.7048375159792775,
      "grad_norm": 6.410808086395264,
      "learning_rate": 2.3273768926476608e-05,
      "loss": 0.665,
      "step": 20100
    },
    {
      "epoch": 2.7452062167799234,
      "grad_norm": 6.476414203643799,
      "learning_rate": 2.286454781066703e-05,
      "loss": 0.6339,
      "step": 20400
    },
    {
      "epoch": 2.7855749175805693,
      "grad_norm": 6.792430877685547,
      "learning_rate": 2.2455326694857457e-05,
      "loss": 0.6243,
      "step": 20700
    },
    {
      "epoch": 2.825943618381215,
      "grad_norm": 6.323462009429932,
      "learning_rate": 2.204610557904788e-05,
      "loss": 0.6139,
      "step": 21000
    },
    {
      "epoch": 2.866312319181861,
      "grad_norm": 5.874839782714844,
      "learning_rate": 2.1636884463238306e-05,
      "loss": 0.5911,
      "step": 21300
    },
    {
      "epoch": 2.906681019982507,
      "grad_norm": 6.340365409851074,
      "learning_rate": 2.122766334742873e-05,
      "loss": 0.5766,
      "step": 21600
    },
    {
      "epoch": 2.947049720783153,
      "grad_norm": 6.194300651550293,
      "learning_rate": 2.0818442231619152e-05,
      "loss": 0.567,
      "step": 21900
    },
    {
      "epoch": 2.987418421583799,
      "grad_norm": 6.31142520904541,
      "learning_rate": 2.0409221115809575e-05,
      "loss": 0.5606,
      "step": 22200
    },
    {
      "epoch": 3.0278544035524457,
      "grad_norm": 5.911204814910889,
      "learning_rate": 2.0001364070386032e-05,
      "loss": 0.5077,
      "step": 22500
    },
    {
      "epoch": 3.0682231043530916,
      "grad_norm": 6.06589937210083,
      "learning_rate": 1.9592142954576455e-05,
      "loss": 0.4809,
      "step": 22800
    },
    {
      "epoch": 3.1085918051537376,
      "grad_norm": 5.514073371887207,
      "learning_rate": 1.918292183876688e-05,
      "loss": 0.4651,
      "step": 23100
    },
    {
      "epoch": 3.1489605059543835,
      "grad_norm": 5.991946697235107,
      "learning_rate": 1.8773700722957304e-05,
      "loss": 0.4634,
      "step": 23400
    },
    {
      "epoch": 3.1893292067550294,
      "grad_norm": 5.506537437438965,
      "learning_rate": 1.836584367753376e-05,
      "loss": 0.4594,
      "step": 23700
    },
    {
      "epoch": 3.2296979075556753,
      "grad_norm": 6.887086868286133,
      "learning_rate": 1.7956622561724184e-05,
      "loss": 0.4502,
      "step": 24000
    },
    {
      "epoch": 3.2700666083563212,
      "grad_norm": 5.336470603942871,
      "learning_rate": 1.754740144591461e-05,
      "loss": 0.441,
      "step": 24300
    },
    {
      "epoch": 3.310435309156967,
      "grad_norm": 5.552666664123535,
      "learning_rate": 1.7138180330105033e-05,
      "loss": 0.4321,
      "step": 24600
    },
    {
      "epoch": 3.3508040099576126,
      "grad_norm": 5.708713054656982,
      "learning_rate": 1.672895921429546e-05,
      "loss": 0.4223,
      "step": 24900
    },
    {
      "epoch": 3.3911727107582585,
      "grad_norm": 5.481934070587158,
      "learning_rate": 1.6319738098485882e-05,
      "loss": 0.4149,
      "step": 25200
    },
    {
      "epoch": 3.4315414115589045,
      "grad_norm": 6.054896354675293,
      "learning_rate": 1.591051698267631e-05,
      "loss": 0.4075,
      "step": 25500
    },
    {
      "epoch": 3.4719101123595504,
      "grad_norm": 5.17155122756958,
      "learning_rate": 1.5501295866866732e-05,
      "loss": 0.3932,
      "step": 25800
    },
    {
      "epoch": 3.5122788131601963,
      "grad_norm": 5.001200199127197,
      "learning_rate": 1.5092074751057156e-05,
      "loss": 0.3893,
      "step": 26100
    },
    {
      "epoch": 3.552647513960842,
      "grad_norm": 6.566178798675537,
      "learning_rate": 1.468285363524758e-05,
      "loss": 0.3863,
      "step": 26400
    },
    {
      "epoch": 3.593016214761488,
      "grad_norm": 4.967824459075928,
      "learning_rate": 1.4273632519438004e-05,
      "loss": 0.373,
      "step": 26700
    },
    {
      "epoch": 3.633384915562134,
      "grad_norm": 5.059580326080322,
      "learning_rate": 1.3864411403628427e-05,
      "loss": 0.3655,
      "step": 27000
    },
    {
      "epoch": 3.67375361636278,
      "grad_norm": 5.326979160308838,
      "learning_rate": 1.3455190287818853e-05,
      "loss": 0.3597,
      "step": 27300
    },
    {
      "epoch": 3.714122317163426,
      "grad_norm": 5.713961124420166,
      "learning_rate": 1.3045969172009276e-05,
      "loss": 0.351,
      "step": 27600
    },
    {
      "epoch": 3.754491017964072,
      "grad_norm": 5.399428844451904,
      "learning_rate": 1.2638112126585733e-05,
      "loss": 0.3447,
      "step": 27900
    },
    {
      "epoch": 3.7948597187647177,
      "grad_norm": 5.643871307373047,
      "learning_rate": 1.2228891010776158e-05,
      "loss": 0.342,
      "step": 28200
    },
    {
      "epoch": 3.8352284195653636,
      "grad_norm": 5.5357747077941895,
      "learning_rate": 1.181966989496658e-05,
      "loss": 0.3312,
      "step": 28500
    },
    {
      "epoch": 3.8755971203660096,
      "grad_norm": 5.6326584815979,
      "learning_rate": 1.1410448779157005e-05,
      "loss": 0.3203,
      "step": 28800
    },
    {
      "epoch": 3.9159658211666555,
      "grad_norm": 5.983763694763184,
      "learning_rate": 1.100122766334743e-05,
      "loss": 0.3131,
      "step": 29100
    },
    {
      "epoch": 3.9563345219673014,
      "grad_norm": 5.726297855377197,
      "learning_rate": 1.0592006547537855e-05,
      "loss": 0.3105,
      "step": 29400
    },
    {
      "epoch": 3.9967032227679473,
      "grad_norm": 4.7484002113342285,
      "learning_rate": 1.018414950211431e-05,
      "loss": 0.3051,
      "step": 29700
    },
    {
      "epoch": 4.037139204736595,
      "grad_norm": 5.7511467933654785,
      "learning_rate": 9.774928386304734e-06,
      "loss": 0.2742,
      "step": 30000
    },
    {
      "epoch": 4.0775079055372405,
      "grad_norm": 5.5173187255859375,
      "learning_rate": 9.365707270495159e-06,
      "loss": 0.2636,
      "step": 30300
    },
    {
      "epoch": 4.117876606337886,
      "grad_norm": 4.812607765197754,
      "learning_rate": 8.956486154685582e-06,
      "loss": 0.2596,
      "step": 30600
    },
    {
      "epoch": 4.158245307138532,
      "grad_norm": 5.58083963394165,
      "learning_rate": 8.547265038876006e-06,
      "loss": 0.2596,
      "step": 30900
    },
    {
      "epoch": 4.198614007939177,
      "grad_norm": 6.447590351104736,
      "learning_rate": 8.138043923066431e-06,
      "loss": 0.252,
      "step": 31200
    },
    {
      "epoch": 4.238982708739823,
      "grad_norm": 4.772024154663086,
      "learning_rate": 7.728822807256856e-06,
      "loss": 0.2481,
      "step": 31500
    },
    {
      "epoch": 4.279351409540469,
      "grad_norm": 4.6637420654296875,
      "learning_rate": 7.320965761833311e-06,
      "loss": 0.2498,
      "step": 31800
    },
    {
      "epoch": 4.319720110341115,
      "grad_norm": 4.746549129486084,
      "learning_rate": 6.911744646023735e-06,
      "loss": 0.2419,
      "step": 32100
    },
    {
      "epoch": 4.360088811141761,
      "grad_norm": 4.768270969390869,
      "learning_rate": 6.502523530214159e-06,
      "loss": 0.2345,
      "step": 32400
    },
    {
      "epoch": 4.400457511942407,
      "grad_norm": 4.978808879852295,
      "learning_rate": 6.093302414404584e-06,
      "loss": 0.2291,
      "step": 32700
    },
    {
      "epoch": 4.440826212743053,
      "grad_norm": 4.573496341705322,
      "learning_rate": 5.684081298595008e-06,
      "loss": 0.2299,
      "step": 33000
    },
    {
      "epoch": 4.481194913543699,
      "grad_norm": 5.130601406097412,
      "learning_rate": 5.274860182785432e-06,
      "loss": 0.2239,
      "step": 33300
    },
    {
      "epoch": 4.521563614344345,
      "grad_norm": 4.455571174621582,
      "learning_rate": 4.865639066975856e-06,
      "loss": 0.2197,
      "step": 33600
    },
    {
      "epoch": 4.561932315144991,
      "grad_norm": 4.464422225952148,
      "learning_rate": 4.457782021552313e-06,
      "loss": 0.2184,
      "step": 33900
    },
    {
      "epoch": 4.602301015945637,
      "grad_norm": 4.77389669418335,
      "learning_rate": 4.0485609057427365e-06,
      "loss": 0.2133,
      "step": 34200
    },
    {
      "epoch": 4.6426697167462825,
      "grad_norm": 4.527365207672119,
      "learning_rate": 3.6393397899331604e-06,
      "loss": 0.2071,
      "step": 34500
    },
    {
      "epoch": 4.683038417546928,
      "grad_norm": 4.540182590484619,
      "learning_rate": 3.2301186741235846e-06,
      "loss": 0.202,
      "step": 34800
    },
    {
      "epoch": 4.723407118347574,
      "grad_norm": 5.098616123199463,
      "learning_rate": 2.820897558314009e-06,
      "loss": 0.2011,
      "step": 35100
    },
    {
      "epoch": 4.76377581914822,
      "grad_norm": 4.6422905921936035,
      "learning_rate": 2.411676442504433e-06,
      "loss": 0.2017,
      "step": 35400
    },
    {
      "epoch": 4.804144519948866,
      "grad_norm": 3.9184505939483643,
      "learning_rate": 2.0038193970808897e-06,
      "loss": 0.2003,
      "step": 35700
    },
    {
      "epoch": 4.844513220749512,
      "grad_norm": 4.729336738586426,
      "learning_rate": 1.5945982812713137e-06,
      "loss": 0.1977,
      "step": 36000
    },
    {
      "epoch": 4.884881921550158,
      "grad_norm": 4.059545516967773,
      "learning_rate": 1.185377165461738e-06,
      "loss": 0.194,
      "step": 36300
    },
    {
      "epoch": 4.925250622350804,
      "grad_norm": 5.020177841186523,
      "learning_rate": 7.761560496521621e-07,
      "loss": 0.1925,
      "step": 36600
    },
    {
      "epoch": 4.96561932315145,
      "grad_norm": 5.328958034515381,
      "learning_rate": 3.669349338425863e-07,
      "loss": 0.1917,
      "step": 36900
    }
  ],
  "logging_steps": 300,
  "max_steps": 37155,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.8312700950987776e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
