{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.999654146780107,
  "eval_steps": 500,
  "global_step": 36140,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013834128795739089,
      "grad_norm": 25.16472625732422,
      "learning_rate": 9.900000000000002e-06,
      "loss": 10.7618,
      "step": 100
    },
    {
      "epoch": 0.027668257591478177,
      "grad_norm": 23.24288558959961,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 9.049,
      "step": 200
    },
    {
      "epoch": 0.04150238638721727,
      "grad_norm": 12.52774429321289,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 7.9959,
      "step": 300
    },
    {
      "epoch": 0.055336515182956354,
      "grad_norm": 8.756865501403809,
      "learning_rate": 3.99e-05,
      "loss": 7.269,
      "step": 400
    },
    {
      "epoch": 0.06917064397869545,
      "grad_norm": 7.979457855224609,
      "learning_rate": 4.99e-05,
      "loss": 6.7863,
      "step": 500
    },
    {
      "epoch": 0.08300477277443453,
      "grad_norm": 6.947065353393555,
      "learning_rate": 4.986111111111111e-05,
      "loss": 6.4319,
      "step": 600
    },
    {
      "epoch": 0.09683890157017362,
      "grad_norm": 6.132612228393555,
      "learning_rate": 4.972081930415264e-05,
      "loss": 6.1103,
      "step": 700
    },
    {
      "epoch": 0.11067303036591271,
      "grad_norm": 6.409430503845215,
      "learning_rate": 4.958052749719417e-05,
      "loss": 5.8089,
      "step": 800
    },
    {
      "epoch": 0.1245071591616518,
      "grad_norm": 7.417187213897705,
      "learning_rate": 4.9440235690235694e-05,
      "loss": 5.552,
      "step": 900
    },
    {
      "epoch": 0.1383412879573909,
      "grad_norm": 5.9764628410339355,
      "learning_rate": 4.929994388327722e-05,
      "loss": 5.3306,
      "step": 1000
    },
    {
      "epoch": 0.15217541675312998,
      "grad_norm": 6.557807922363281,
      "learning_rate": 4.9159652076318744e-05,
      "loss": 5.1554,
      "step": 1100
    },
    {
      "epoch": 0.16600954554886907,
      "grad_norm": 5.717743873596191,
      "learning_rate": 4.901936026936027e-05,
      "loss": 4.9907,
      "step": 1200
    },
    {
      "epoch": 0.17984367434460816,
      "grad_norm": 5.571457862854004,
      "learning_rate": 4.88790684624018e-05,
      "loss": 4.8611,
      "step": 1300
    },
    {
      "epoch": 0.19367780314034724,
      "grad_norm": 4.89622163772583,
      "learning_rate": 4.8738776655443327e-05,
      "loss": 4.7313,
      "step": 1400
    },
    {
      "epoch": 0.20751193193608633,
      "grad_norm": 5.213212013244629,
      "learning_rate": 4.859848484848485e-05,
      "loss": 4.5658,
      "step": 1500
    },
    {
      "epoch": 0.22134606073182542,
      "grad_norm": 5.465956687927246,
      "learning_rate": 4.845819304152638e-05,
      "loss": 4.4801,
      "step": 1600
    },
    {
      "epoch": 0.2351801895275645,
      "grad_norm": 5.513957977294922,
      "learning_rate": 4.83179012345679e-05,
      "loss": 4.3718,
      "step": 1700
    },
    {
      "epoch": 0.2490143183233036,
      "grad_norm": 5.432742595672607,
      "learning_rate": 4.8177609427609434e-05,
      "loss": 4.3,
      "step": 1800
    },
    {
      "epoch": 0.2628484471190427,
      "grad_norm": 4.856459140777588,
      "learning_rate": 4.803731762065096e-05,
      "loss": 4.2356,
      "step": 1900
    },
    {
      "epoch": 0.2766825759147818,
      "grad_norm": 4.687174320220947,
      "learning_rate": 4.7897025813692484e-05,
      "loss": 4.1522,
      "step": 2000
    },
    {
      "epoch": 0.2905167047105209,
      "grad_norm": 5.055035591125488,
      "learning_rate": 4.775673400673401e-05,
      "loss": 4.064,
      "step": 2100
    },
    {
      "epoch": 0.30435083350625997,
      "grad_norm": 4.803167343139648,
      "learning_rate": 4.7616442199775535e-05,
      "loss": 4.0066,
      "step": 2200
    },
    {
      "epoch": 0.31818496230199905,
      "grad_norm": 5.049839019775391,
      "learning_rate": 4.747615039281706e-05,
      "loss": 3.9276,
      "step": 2300
    },
    {
      "epoch": 0.33201909109773814,
      "grad_norm": 4.9754533767700195,
      "learning_rate": 4.733585858585859e-05,
      "loss": 3.9051,
      "step": 2400
    },
    {
      "epoch": 0.3458532198934772,
      "grad_norm": 5.1123552322387695,
      "learning_rate": 4.719556677890011e-05,
      "loss": 3.836,
      "step": 2500
    },
    {
      "epoch": 0.3596873486892163,
      "grad_norm": 5.006784915924072,
      "learning_rate": 4.705527497194164e-05,
      "loss": 3.7884,
      "step": 2600
    },
    {
      "epoch": 0.3735214774849554,
      "grad_norm": 4.7941999435424805,
      "learning_rate": 4.691498316498317e-05,
      "loss": 3.7334,
      "step": 2700
    },
    {
      "epoch": 0.3873556062806945,
      "grad_norm": 5.107181549072266,
      "learning_rate": 4.677469135802469e-05,
      "loss": 3.6829,
      "step": 2800
    },
    {
      "epoch": 0.4011897350764336,
      "grad_norm": 5.19609260559082,
      "learning_rate": 4.6634399551066224e-05,
      "loss": 3.6233,
      "step": 2900
    },
    {
      "epoch": 0.41502386387217266,
      "grad_norm": 5.001264572143555,
      "learning_rate": 4.649410774410774e-05,
      "loss": 3.6108,
      "step": 3000
    },
    {
      "epoch": 0.42885799266791175,
      "grad_norm": 5.308812618255615,
      "learning_rate": 4.6353815937149275e-05,
      "loss": 3.5157,
      "step": 3100
    },
    {
      "epoch": 0.44269212146365083,
      "grad_norm": 4.750976085662842,
      "learning_rate": 4.62135241301908e-05,
      "loss": 3.5129,
      "step": 3200
    },
    {
      "epoch": 0.4565262502593899,
      "grad_norm": 4.826512336730957,
      "learning_rate": 4.6073232323232325e-05,
      "loss": 3.4512,
      "step": 3300
    },
    {
      "epoch": 0.470360379055129,
      "grad_norm": 4.804762840270996,
      "learning_rate": 4.593294051627385e-05,
      "loss": 3.4285,
      "step": 3400
    },
    {
      "epoch": 0.4841945078508681,
      "grad_norm": 5.11376428604126,
      "learning_rate": 4.5792648709315375e-05,
      "loss": 3.3676,
      "step": 3500
    },
    {
      "epoch": 0.4980286366466072,
      "grad_norm": 5.135071277618408,
      "learning_rate": 4.56523569023569e-05,
      "loss": 3.3538,
      "step": 3600
    },
    {
      "epoch": 0.5118627654423463,
      "grad_norm": 4.6575822830200195,
      "learning_rate": 4.551206509539843e-05,
      "loss": 3.3144,
      "step": 3700
    },
    {
      "epoch": 0.5256968942380854,
      "grad_norm": 4.830664157867432,
      "learning_rate": 4.537177328843996e-05,
      "loss": 3.2759,
      "step": 3800
    },
    {
      "epoch": 0.5395310230338245,
      "grad_norm": 5.2333550453186035,
      "learning_rate": 4.523148148148148e-05,
      "loss": 3.1911,
      "step": 3900
    },
    {
      "epoch": 0.5533651518295636,
      "grad_norm": 5.296065807342529,
      "learning_rate": 4.509118967452301e-05,
      "loss": 3.1868,
      "step": 4000
    },
    {
      "epoch": 0.5671992806253027,
      "grad_norm": 5.1277923583984375,
      "learning_rate": 4.495089786756453e-05,
      "loss": 3.1348,
      "step": 4100
    },
    {
      "epoch": 0.5810334094210418,
      "grad_norm": 5.634518146514893,
      "learning_rate": 4.4810606060606065e-05,
      "loss": 3.1023,
      "step": 4200
    },
    {
      "epoch": 0.5948675382167808,
      "grad_norm": 5.260359287261963,
      "learning_rate": 4.467031425364759e-05,
      "loss": 3.0462,
      "step": 4300
    },
    {
      "epoch": 0.6087016670125199,
      "grad_norm": 5.108173847198486,
      "learning_rate": 4.4530022446689116e-05,
      "loss": 3.0506,
      "step": 4400
    },
    {
      "epoch": 0.622535795808259,
      "grad_norm": 4.847431659698486,
      "learning_rate": 4.438973063973064e-05,
      "loss": 2.976,
      "step": 4500
    },
    {
      "epoch": 0.6363699246039981,
      "grad_norm": 5.1014227867126465,
      "learning_rate": 4.4249438832772166e-05,
      "loss": 2.9283,
      "step": 4600
    },
    {
      "epoch": 0.6502040533997372,
      "grad_norm": 4.9156975746154785,
      "learning_rate": 4.41091470258137e-05,
      "loss": 2.8757,
      "step": 4700
    },
    {
      "epoch": 0.6640381821954763,
      "grad_norm": 4.915997505187988,
      "learning_rate": 4.396885521885522e-05,
      "loss": 2.8818,
      "step": 4800
    },
    {
      "epoch": 0.6778723109912154,
      "grad_norm": 4.751307010650635,
      "learning_rate": 4.382856341189674e-05,
      "loss": 2.8412,
      "step": 4900
    },
    {
      "epoch": 0.6917064397869545,
      "grad_norm": 5.170293807983398,
      "learning_rate": 4.368827160493827e-05,
      "loss": 2.8082,
      "step": 5000
    },
    {
      "epoch": 0.7055405685826935,
      "grad_norm": 5.2604241371154785,
      "learning_rate": 4.35479797979798e-05,
      "loss": 2.7522,
      "step": 5100
    },
    {
      "epoch": 0.7193746973784326,
      "grad_norm": 4.9990010261535645,
      "learning_rate": 4.340768799102133e-05,
      "loss": 2.7186,
      "step": 5200
    },
    {
      "epoch": 0.7332088261741717,
      "grad_norm": 5.02250337600708,
      "learning_rate": 4.3267396184062856e-05,
      "loss": 2.7121,
      "step": 5300
    },
    {
      "epoch": 0.7470429549699108,
      "grad_norm": 5.304316997528076,
      "learning_rate": 4.3127104377104374e-05,
      "loss": 2.6425,
      "step": 5400
    },
    {
      "epoch": 0.7608770837656499,
      "grad_norm": 4.894322395324707,
      "learning_rate": 4.2986812570145906e-05,
      "loss": 2.6577,
      "step": 5500
    },
    {
      "epoch": 0.774711212561389,
      "grad_norm": 5.173647880554199,
      "learning_rate": 4.284652076318743e-05,
      "loss": 2.5948,
      "step": 5600
    },
    {
      "epoch": 0.7885453413571281,
      "grad_norm": 5.181605815887451,
      "learning_rate": 4.270622895622896e-05,
      "loss": 2.5801,
      "step": 5700
    },
    {
      "epoch": 0.8023794701528671,
      "grad_norm": 6.121105670928955,
      "learning_rate": 4.256593714927049e-05,
      "loss": 2.5451,
      "step": 5800
    },
    {
      "epoch": 0.8162135989486062,
      "grad_norm": 5.774332046508789,
      "learning_rate": 4.242564534231201e-05,
      "loss": 2.5216,
      "step": 5900
    },
    {
      "epoch": 0.8300477277443453,
      "grad_norm": 5.429629325866699,
      "learning_rate": 4.228535353535354e-05,
      "loss": 2.4994,
      "step": 6000
    },
    {
      "epoch": 0.8438818565400844,
      "grad_norm": 5.433135509490967,
      "learning_rate": 4.2145061728395064e-05,
      "loss": 2.4483,
      "step": 6100
    },
    {
      "epoch": 0.8577159853358235,
      "grad_norm": 5.537074565887451,
      "learning_rate": 4.200476992143659e-05,
      "loss": 2.4094,
      "step": 6200
    },
    {
      "epoch": 0.8715501141315626,
      "grad_norm": 5.973832130432129,
      "learning_rate": 4.1864478114478114e-05,
      "loss": 2.3996,
      "step": 6300
    },
    {
      "epoch": 0.8853842429273017,
      "grad_norm": 5.633718013763428,
      "learning_rate": 4.172418630751964e-05,
      "loss": 2.3445,
      "step": 6400
    },
    {
      "epoch": 0.8992183717230408,
      "grad_norm": 5.392309665679932,
      "learning_rate": 4.158389450056117e-05,
      "loss": 2.3077,
      "step": 6500
    },
    {
      "epoch": 0.9130525005187798,
      "grad_norm": 5.5942769050598145,
      "learning_rate": 4.1443602693602696e-05,
      "loss": 2.3051,
      "step": 6600
    },
    {
      "epoch": 0.9268866293145189,
      "grad_norm": 5.418362617492676,
      "learning_rate": 4.130331088664422e-05,
      "loss": 2.2359,
      "step": 6700
    },
    {
      "epoch": 0.940720758110258,
      "grad_norm": 5.937961101531982,
      "learning_rate": 4.116301907968575e-05,
      "loss": 2.2123,
      "step": 6800
    },
    {
      "epoch": 0.9545548869059971,
      "grad_norm": 5.605092525482178,
      "learning_rate": 4.102272727272727e-05,
      "loss": 2.1804,
      "step": 6900
    },
    {
      "epoch": 0.9683890157017362,
      "grad_norm": 5.61198091506958,
      "learning_rate": 4.0882435465768804e-05,
      "loss": 2.165,
      "step": 7000
    },
    {
      "epoch": 0.9822231444974753,
      "grad_norm": 5.737157821655273,
      "learning_rate": 4.074214365881033e-05,
      "loss": 2.1284,
      "step": 7100
    },
    {
      "epoch": 0.9960572732932144,
      "grad_norm": 6.003175258636475,
      "learning_rate": 4.0601851851851854e-05,
      "loss": 2.1147,
      "step": 7200
    },
    {
      "epoch": 1.0099605727329322,
      "grad_norm": 6.000420093536377,
      "learning_rate": 4.046156004489338e-05,
      "loss": 2.0465,
      "step": 7300
    },
    {
      "epoch": 1.0237947015286712,
      "grad_norm": 5.685454845428467,
      "learning_rate": 4.0321268237934905e-05,
      "loss": 1.9907,
      "step": 7400
    },
    {
      "epoch": 1.0376288303244103,
      "grad_norm": 5.794159889221191,
      "learning_rate": 4.018097643097643e-05,
      "loss": 1.968,
      "step": 7500
    },
    {
      "epoch": 1.0514629591201494,
      "grad_norm": 5.8921356201171875,
      "learning_rate": 4.004068462401796e-05,
      "loss": 1.9432,
      "step": 7600
    },
    {
      "epoch": 1.0652970879158885,
      "grad_norm": 5.922020435333252,
      "learning_rate": 3.990039281705949e-05,
      "loss": 1.9208,
      "step": 7700
    },
    {
      "epoch": 1.0791312167116276,
      "grad_norm": 5.887676239013672,
      "learning_rate": 3.976010101010101e-05,
      "loss": 1.9192,
      "step": 7800
    },
    {
      "epoch": 1.0929653455073667,
      "grad_norm": 5.904050827026367,
      "learning_rate": 3.961980920314254e-05,
      "loss": 1.905,
      "step": 7900
    },
    {
      "epoch": 1.1067994743031058,
      "grad_norm": 5.486944675445557,
      "learning_rate": 3.947951739618406e-05,
      "loss": 1.8836,
      "step": 8000
    },
    {
      "epoch": 1.1206336030988449,
      "grad_norm": 5.999889850616455,
      "learning_rate": 3.9339225589225594e-05,
      "loss": 1.8555,
      "step": 8100
    },
    {
      "epoch": 1.134467731894584,
      "grad_norm": 5.398906707763672,
      "learning_rate": 3.9200336700336704e-05,
      "loss": 1.832,
      "step": 8200
    },
    {
      "epoch": 1.148301860690323,
      "grad_norm": 6.2012104988098145,
      "learning_rate": 3.906004489337823e-05,
      "loss": 1.789,
      "step": 8300
    },
    {
      "epoch": 1.1621359894860621,
      "grad_norm": 5.5211405754089355,
      "learning_rate": 3.8919753086419754e-05,
      "loss": 1.7836,
      "step": 8400
    },
    {
      "epoch": 1.1759701182818012,
      "grad_norm": 6.374581813812256,
      "learning_rate": 3.877946127946128e-05,
      "loss": 1.768,
      "step": 8500
    },
    {
      "epoch": 1.1898042470775403,
      "grad_norm": 6.275337219238281,
      "learning_rate": 3.8639169472502804e-05,
      "loss": 1.743,
      "step": 8600
    },
    {
      "epoch": 1.2036383758732794,
      "grad_norm": 6.307816505432129,
      "learning_rate": 3.8498877665544336e-05,
      "loss": 1.7217,
      "step": 8700
    },
    {
      "epoch": 1.2174725046690185,
      "grad_norm": 6.166470527648926,
      "learning_rate": 3.835858585858586e-05,
      "loss": 1.7044,
      "step": 8800
    },
    {
      "epoch": 1.2313066334647575,
      "grad_norm": 5.860273838043213,
      "learning_rate": 3.8218294051627386e-05,
      "loss": 1.6735,
      "step": 8900
    },
    {
      "epoch": 1.2451407622604966,
      "grad_norm": 5.925512313842773,
      "learning_rate": 3.807800224466891e-05,
      "loss": 1.6388,
      "step": 9000
    },
    {
      "epoch": 1.2589748910562357,
      "grad_norm": 6.259415626525879,
      "learning_rate": 3.793771043771044e-05,
      "loss": 1.6471,
      "step": 9100
    },
    {
      "epoch": 1.2728090198519748,
      "grad_norm": 6.593967914581299,
      "learning_rate": 3.779741863075197e-05,
      "loss": 1.6391,
      "step": 9200
    },
    {
      "epoch": 1.286643148647714,
      "grad_norm": 6.198666572570801,
      "learning_rate": 3.7657126823793494e-05,
      "loss": 1.5997,
      "step": 9300
    },
    {
      "epoch": 1.300477277443453,
      "grad_norm": 6.394721031188965,
      "learning_rate": 3.751683501683502e-05,
      "loss": 1.5963,
      "step": 9400
    },
    {
      "epoch": 1.314311406239192,
      "grad_norm": 5.837399482727051,
      "learning_rate": 3.7376543209876544e-05,
      "loss": 1.593,
      "step": 9500
    },
    {
      "epoch": 1.3281455350349312,
      "grad_norm": 6.095070838928223,
      "learning_rate": 3.723625140291807e-05,
      "loss": 1.5635,
      "step": 9600
    },
    {
      "epoch": 1.3419796638306702,
      "grad_norm": 6.249611854553223,
      "learning_rate": 3.7095959595959595e-05,
      "loss": 1.5329,
      "step": 9700
    },
    {
      "epoch": 1.3558137926264093,
      "grad_norm": 6.799038887023926,
      "learning_rate": 3.6955667789001127e-05,
      "loss": 1.5274,
      "step": 9800
    },
    {
      "epoch": 1.3696479214221484,
      "grad_norm": 6.641458034515381,
      "learning_rate": 3.681537598204265e-05,
      "loss": 1.5063,
      "step": 9900
    },
    {
      "epoch": 1.3834820502178875,
      "grad_norm": 6.690907955169678,
      "learning_rate": 3.667508417508418e-05,
      "loss": 1.4908,
      "step": 10000
    },
    {
      "epoch": 1.3973161790136266,
      "grad_norm": 5.639688014984131,
      "learning_rate": 3.65347923681257e-05,
      "loss": 1.5049,
      "step": 10100
    },
    {
      "epoch": 1.4111503078093657,
      "grad_norm": 6.110543251037598,
      "learning_rate": 3.639450056116723e-05,
      "loss": 1.483,
      "step": 10200
    },
    {
      "epoch": 1.4249844366051048,
      "grad_norm": 6.004232883453369,
      "learning_rate": 3.625420875420876e-05,
      "loss": 1.4652,
      "step": 10300
    },
    {
      "epoch": 1.4388185654008439,
      "grad_norm": 6.046010494232178,
      "learning_rate": 3.611531986531987e-05,
      "loss": 1.4718,
      "step": 10400
    },
    {
      "epoch": 1.452652694196583,
      "grad_norm": 6.630728244781494,
      "learning_rate": 3.5975028058361394e-05,
      "loss": 1.4389,
      "step": 10500
    },
    {
      "epoch": 1.466486822992322,
      "grad_norm": 6.188145637512207,
      "learning_rate": 3.583473625140292e-05,
      "loss": 1.4102,
      "step": 10600
    },
    {
      "epoch": 1.4803209517880611,
      "grad_norm": 6.22721004486084,
      "learning_rate": 3.5694444444444444e-05,
      "loss": 1.4117,
      "step": 10700
    },
    {
      "epoch": 1.4941550805838002,
      "grad_norm": 6.04557991027832,
      "learning_rate": 3.555415263748597e-05,
      "loss": 1.3856,
      "step": 10800
    },
    {
      "epoch": 1.5079892093795393,
      "grad_norm": 6.197278022766113,
      "learning_rate": 3.54138608305275e-05,
      "loss": 1.3586,
      "step": 10900
    },
    {
      "epoch": 1.5218233381752784,
      "grad_norm": 5.970922946929932,
      "learning_rate": 3.5273569023569026e-05,
      "loss": 1.3828,
      "step": 11000
    },
    {
      "epoch": 1.5356574669710175,
      "grad_norm": 6.113564968109131,
      "learning_rate": 3.513327721661055e-05,
      "loss": 1.3302,
      "step": 11100
    },
    {
      "epoch": 1.5494915957667565,
      "grad_norm": 6.045312881469727,
      "learning_rate": 3.4992985409652077e-05,
      "loss": 1.3264,
      "step": 11200
    },
    {
      "epoch": 1.5633257245624956,
      "grad_norm": 5.966328144073486,
      "learning_rate": 3.48526936026936e-05,
      "loss": 1.316,
      "step": 11300
    },
    {
      "epoch": 1.5771598533582347,
      "grad_norm": 6.326780796051025,
      "learning_rate": 3.4712401795735134e-05,
      "loss": 1.301,
      "step": 11400
    },
    {
      "epoch": 1.5909939821539738,
      "grad_norm": 6.388791561126709,
      "learning_rate": 3.457210998877666e-05,
      "loss": 1.2994,
      "step": 11500
    },
    {
      "epoch": 1.604828110949713,
      "grad_norm": 6.011089324951172,
      "learning_rate": 3.4431818181818184e-05,
      "loss": 1.2744,
      "step": 11600
    },
    {
      "epoch": 1.618662239745452,
      "grad_norm": 6.094967842102051,
      "learning_rate": 3.429152637485971e-05,
      "loss": 1.2547,
      "step": 11700
    },
    {
      "epoch": 1.632496368541191,
      "grad_norm": 5.571987628936768,
      "learning_rate": 3.4151234567901234e-05,
      "loss": 1.2503,
      "step": 11800
    },
    {
      "epoch": 1.6463304973369302,
      "grad_norm": 5.978718280792236,
      "learning_rate": 3.401094276094276e-05,
      "loss": 1.2419,
      "step": 11900
    },
    {
      "epoch": 1.6601646261326692,
      "grad_norm": 6.39376163482666,
      "learning_rate": 3.387065095398429e-05,
      "loss": 1.2381,
      "step": 12000
    },
    {
      "epoch": 1.6739987549284083,
      "grad_norm": 5.622151851654053,
      "learning_rate": 3.373035914702582e-05,
      "loss": 1.1881,
      "step": 12100
    },
    {
      "epoch": 1.6878328837241474,
      "grad_norm": 6.300588607788086,
      "learning_rate": 3.359006734006734e-05,
      "loss": 1.2172,
      "step": 12200
    },
    {
      "epoch": 1.7016670125198865,
      "grad_norm": 6.126545429229736,
      "learning_rate": 3.344977553310887e-05,
      "loss": 1.201,
      "step": 12300
    },
    {
      "epoch": 1.7155011413156256,
      "grad_norm": 7.020421504974365,
      "learning_rate": 3.330948372615039e-05,
      "loss": 1.159,
      "step": 12400
    },
    {
      "epoch": 1.7293352701113647,
      "grad_norm": 6.209599494934082,
      "learning_rate": 3.3169191919191924e-05,
      "loss": 1.1683,
      "step": 12500
    },
    {
      "epoch": 1.7431693989071038,
      "grad_norm": 6.334655284881592,
      "learning_rate": 3.302890011223345e-05,
      "loss": 1.1549,
      "step": 12600
    },
    {
      "epoch": 1.7570035277028428,
      "grad_norm": 5.910346508026123,
      "learning_rate": 3.2888608305274974e-05,
      "loss": 1.135,
      "step": 12700
    },
    {
      "epoch": 1.770837656498582,
      "grad_norm": 5.826834678649902,
      "learning_rate": 3.27483164983165e-05,
      "loss": 1.1341,
      "step": 12800
    },
    {
      "epoch": 1.784671785294321,
      "grad_norm": 7.175501823425293,
      "learning_rate": 3.260942760942761e-05,
      "loss": 1.1255,
      "step": 12900
    },
    {
      "epoch": 1.79850591409006,
      "grad_norm": 5.952853202819824,
      "learning_rate": 3.2469135802469134e-05,
      "loss": 1.0943,
      "step": 13000
    },
    {
      "epoch": 1.8123400428857992,
      "grad_norm": 6.0487565994262695,
      "learning_rate": 3.2328843995510666e-05,
      "loss": 1.0735,
      "step": 13100
    },
    {
      "epoch": 1.8261741716815383,
      "grad_norm": 6.224886894226074,
      "learning_rate": 3.218855218855219e-05,
      "loss": 1.0782,
      "step": 13200
    },
    {
      "epoch": 1.8400083004772774,
      "grad_norm": 6.398498058319092,
      "learning_rate": 3.2048260381593716e-05,
      "loss": 1.0785,
      "step": 13300
    },
    {
      "epoch": 1.8538424292730165,
      "grad_norm": 5.727413177490234,
      "learning_rate": 3.190796857463524e-05,
      "loss": 1.0485,
      "step": 13400
    },
    {
      "epoch": 1.8676765580687555,
      "grad_norm": 5.708187580108643,
      "learning_rate": 3.176767676767677e-05,
      "loss": 1.0594,
      "step": 13500
    },
    {
      "epoch": 1.8815106868644946,
      "grad_norm": 6.617132663726807,
      "learning_rate": 3.16273849607183e-05,
      "loss": 1.0453,
      "step": 13600
    },
    {
      "epoch": 1.8953448156602337,
      "grad_norm": 6.046110153198242,
      "learning_rate": 3.1487093153759824e-05,
      "loss": 1.025,
      "step": 13700
    },
    {
      "epoch": 1.9091789444559728,
      "grad_norm": 5.720885753631592,
      "learning_rate": 3.134680134680135e-05,
      "loss": 1.015,
      "step": 13800
    },
    {
      "epoch": 1.923013073251712,
      "grad_norm": 6.448936462402344,
      "learning_rate": 3.1206509539842874e-05,
      "loss": 1.0119,
      "step": 13900
    },
    {
      "epoch": 1.936847202047451,
      "grad_norm": 6.746499538421631,
      "learning_rate": 3.10662177328844e-05,
      "loss": 0.9874,
      "step": 14000
    },
    {
      "epoch": 1.95068133084319,
      "grad_norm": 6.331389427185059,
      "learning_rate": 3.0925925925925924e-05,
      "loss": 1.0028,
      "step": 14100
    },
    {
      "epoch": 1.9645154596389292,
      "grad_norm": 5.797363758087158,
      "learning_rate": 3.0785634118967456e-05,
      "loss": 0.9896,
      "step": 14200
    },
    {
      "epoch": 1.9783495884346682,
      "grad_norm": 6.186957836151123,
      "learning_rate": 3.064534231200898e-05,
      "loss": 0.9773,
      "step": 14300
    },
    {
      "epoch": 1.9921837172304073,
      "grad_norm": 6.18734073638916,
      "learning_rate": 3.050505050505051e-05,
      "loss": 0.98,
      "step": 14400
    },
    {
      "epoch": 2.0060870166701252,
      "grad_norm": 5.5518951416015625,
      "learning_rate": 3.0364758698092032e-05,
      "loss": 0.9407,
      "step": 14500
    },
    {
      "epoch": 2.0199211454658643,
      "grad_norm": 5.866950511932373,
      "learning_rate": 3.0224466891133557e-05,
      "loss": 0.8743,
      "step": 14600
    },
    {
      "epoch": 2.0337552742616034,
      "grad_norm": 5.247119426727295,
      "learning_rate": 3.0084175084175086e-05,
      "loss": 0.8824,
      "step": 14700
    },
    {
      "epoch": 2.0475894030573425,
      "grad_norm": 5.899742603302002,
      "learning_rate": 2.994388327721661e-05,
      "loss": 0.8621,
      "step": 14800
    },
    {
      "epoch": 2.0614235318530816,
      "grad_norm": 5.84663724899292,
      "learning_rate": 2.980359147025814e-05,
      "loss": 0.864,
      "step": 14900
    },
    {
      "epoch": 2.0752576606488207,
      "grad_norm": 5.951037406921387,
      "learning_rate": 2.9663299663299665e-05,
      "loss": 0.8721,
      "step": 15000
    },
    {
      "epoch": 2.0890917894445598,
      "grad_norm": 5.9608473777771,
      "learning_rate": 2.952300785634119e-05,
      "loss": 0.8401,
      "step": 15100
    },
    {
      "epoch": 2.102925918240299,
      "grad_norm": 6.112856388092041,
      "learning_rate": 2.9382716049382718e-05,
      "loss": 0.8488,
      "step": 15200
    },
    {
      "epoch": 2.116760047036038,
      "grad_norm": 6.012358665466309,
      "learning_rate": 2.9242424242424243e-05,
      "loss": 0.8447,
      "step": 15300
    },
    {
      "epoch": 2.130594175831777,
      "grad_norm": 5.894104957580566,
      "learning_rate": 2.910213243546577e-05,
      "loss": 0.8349,
      "step": 15400
    },
    {
      "epoch": 2.144428304627516,
      "grad_norm": 6.209616661071777,
      "learning_rate": 2.8961840628507297e-05,
      "loss": 0.8187,
      "step": 15500
    },
    {
      "epoch": 2.158262433423255,
      "grad_norm": 5.968942165374756,
      "learning_rate": 2.8821548821548822e-05,
      "loss": 0.8213,
      "step": 15600
    },
    {
      "epoch": 2.1720965622189943,
      "grad_norm": 6.024006366729736,
      "learning_rate": 2.868125701459035e-05,
      "loss": 0.8219,
      "step": 15700
    },
    {
      "epoch": 2.1859306910147334,
      "grad_norm": 6.271935939788818,
      "learning_rate": 2.8540965207631876e-05,
      "loss": 0.7975,
      "step": 15800
    },
    {
      "epoch": 2.1997648198104724,
      "grad_norm": 5.9546637535095215,
      "learning_rate": 2.84006734006734e-05,
      "loss": 0.7998,
      "step": 15900
    },
    {
      "epoch": 2.2135989486062115,
      "grad_norm": 6.048415184020996,
      "learning_rate": 2.826038159371493e-05,
      "loss": 0.8022,
      "step": 16000
    },
    {
      "epoch": 2.2274330774019506,
      "grad_norm": 6.727811813354492,
      "learning_rate": 2.8120089786756455e-05,
      "loss": 0.7912,
      "step": 16100
    },
    {
      "epoch": 2.2412672061976897,
      "grad_norm": 5.609253406524658,
      "learning_rate": 2.7979797979797984e-05,
      "loss": 0.7885,
      "step": 16200
    },
    {
      "epoch": 2.255101334993429,
      "grad_norm": 6.101441383361816,
      "learning_rate": 2.783950617283951e-05,
      "loss": 0.7679,
      "step": 16300
    },
    {
      "epoch": 2.268935463789168,
      "grad_norm": 5.473152160644531,
      "learning_rate": 2.769921436588103e-05,
      "loss": 0.7857,
      "step": 16400
    },
    {
      "epoch": 2.282769592584907,
      "grad_norm": 5.910261154174805,
      "learning_rate": 2.7558922558922562e-05,
      "loss": 0.7725,
      "step": 16500
    },
    {
      "epoch": 2.296603721380646,
      "grad_norm": 6.3421196937561035,
      "learning_rate": 2.7418630751964088e-05,
      "loss": 0.7423,
      "step": 16600
    },
    {
      "epoch": 2.310437850176385,
      "grad_norm": 5.5853352546691895,
      "learning_rate": 2.727833894500561e-05,
      "loss": 0.7563,
      "step": 16700
    },
    {
      "epoch": 2.3242719789721242,
      "grad_norm": 5.565927982330322,
      "learning_rate": 2.713804713804714e-05,
      "loss": 0.7505,
      "step": 16800
    },
    {
      "epoch": 2.3381061077678633,
      "grad_norm": 5.5117011070251465,
      "learning_rate": 2.699915824915825e-05,
      "loss": 0.7405,
      "step": 16900
    },
    {
      "epoch": 2.3519402365636024,
      "grad_norm": 6.095532417297363,
      "learning_rate": 2.6858866442199776e-05,
      "loss": 0.7348,
      "step": 17000
    },
    {
      "epoch": 2.3657743653593415,
      "grad_norm": 5.2978339195251465,
      "learning_rate": 2.6718574635241304e-05,
      "loss": 0.7293,
      "step": 17100
    },
    {
      "epoch": 2.3796084941550806,
      "grad_norm": 5.877203464508057,
      "learning_rate": 2.657828282828283e-05,
      "loss": 0.7316,
      "step": 17200
    },
    {
      "epoch": 2.3934426229508197,
      "grad_norm": 5.754256725311279,
      "learning_rate": 2.6437991021324355e-05,
      "loss": 0.7268,
      "step": 17300
    },
    {
      "epoch": 2.4072767517465588,
      "grad_norm": 5.540423393249512,
      "learning_rate": 2.6297699214365883e-05,
      "loss": 0.7254,
      "step": 17400
    },
    {
      "epoch": 2.421110880542298,
      "grad_norm": 6.820533275604248,
      "learning_rate": 2.615740740740741e-05,
      "loss": 0.7211,
      "step": 17500
    },
    {
      "epoch": 2.434945009338037,
      "grad_norm": 5.877835273742676,
      "learning_rate": 2.6017115600448934e-05,
      "loss": 0.7125,
      "step": 17600
    },
    {
      "epoch": 2.448779138133776,
      "grad_norm": 5.774829387664795,
      "learning_rate": 2.5876823793490462e-05,
      "loss": 0.69,
      "step": 17700
    },
    {
      "epoch": 2.462613266929515,
      "grad_norm": 5.272575378417969,
      "learning_rate": 2.5736531986531987e-05,
      "loss": 0.7055,
      "step": 17800
    },
    {
      "epoch": 2.476447395725254,
      "grad_norm": 5.695120811462402,
      "learning_rate": 2.5596240179573516e-05,
      "loss": 0.6833,
      "step": 17900
    },
    {
      "epoch": 2.4902815245209933,
      "grad_norm": 5.929096698760986,
      "learning_rate": 2.545594837261504e-05,
      "loss": 0.6799,
      "step": 18000
    },
    {
      "epoch": 2.5041156533167324,
      "grad_norm": 5.986080646514893,
      "learning_rate": 2.5315656565656566e-05,
      "loss": 0.6776,
      "step": 18100
    },
    {
      "epoch": 2.5179497821124714,
      "grad_norm": 6.14808988571167,
      "learning_rate": 2.5175364758698095e-05,
      "loss": 0.6667,
      "step": 18200
    },
    {
      "epoch": 2.5317839109082105,
      "grad_norm": 5.521708011627197,
      "learning_rate": 2.503507295173962e-05,
      "loss": 0.6473,
      "step": 18300
    },
    {
      "epoch": 2.5456180397039496,
      "grad_norm": 6.430391311645508,
      "learning_rate": 2.4894781144781145e-05,
      "loss": 0.641,
      "step": 18400
    },
    {
      "epoch": 2.5594521684996887,
      "grad_norm": 5.769807815551758,
      "learning_rate": 2.4754489337822674e-05,
      "loss": 0.6527,
      "step": 18500
    },
    {
      "epoch": 2.573286297295428,
      "grad_norm": 5.68735933303833,
      "learning_rate": 2.46141975308642e-05,
      "loss": 0.6514,
      "step": 18600
    },
    {
      "epoch": 2.587120426091167,
      "grad_norm": 6.0357489585876465,
      "learning_rate": 2.4473905723905724e-05,
      "loss": 0.6649,
      "step": 18700
    },
    {
      "epoch": 2.600954554886906,
      "grad_norm": 5.463881492614746,
      "learning_rate": 2.4333613916947253e-05,
      "loss": 0.6348,
      "step": 18800
    },
    {
      "epoch": 2.614788683682645,
      "grad_norm": 5.9130024909973145,
      "learning_rate": 2.4194725028058362e-05,
      "loss": 0.6473,
      "step": 18900
    },
    {
      "epoch": 2.628622812478384,
      "grad_norm": 5.992900371551514,
      "learning_rate": 2.405443322109989e-05,
      "loss": 0.6194,
      "step": 19000
    },
    {
      "epoch": 2.6424569412741232,
      "grad_norm": 5.490206718444824,
      "learning_rate": 2.3914141414141416e-05,
      "loss": 0.6241,
      "step": 19100
    },
    {
      "epoch": 2.6562910700698623,
      "grad_norm": 5.838356971740723,
      "learning_rate": 2.377384960718294e-05,
      "loss": 0.6314,
      "step": 19200
    },
    {
      "epoch": 2.6701251988656014,
      "grad_norm": 5.1879730224609375,
      "learning_rate": 2.363355780022447e-05,
      "loss": 0.6112,
      "step": 19300
    },
    {
      "epoch": 2.6839593276613405,
      "grad_norm": 6.286222457885742,
      "learning_rate": 2.3493265993265994e-05,
      "loss": 0.5979,
      "step": 19400
    },
    {
      "epoch": 2.6977934564570796,
      "grad_norm": 5.693620204925537,
      "learning_rate": 2.3352974186307523e-05,
      "loss": 0.6139,
      "step": 19500
    },
    {
      "epoch": 2.7116275852528187,
      "grad_norm": 5.851102352142334,
      "learning_rate": 2.3212682379349045e-05,
      "loss": 0.6009,
      "step": 19600
    },
    {
      "epoch": 2.7254617140485577,
      "grad_norm": 5.267277717590332,
      "learning_rate": 2.3072390572390573e-05,
      "loss": 0.582,
      "step": 19700
    },
    {
      "epoch": 2.739295842844297,
      "grad_norm": 5.693506240844727,
      "learning_rate": 2.29320987654321e-05,
      "loss": 0.5946,
      "step": 19800
    },
    {
      "epoch": 2.753129971640036,
      "grad_norm": 6.051726818084717,
      "learning_rate": 2.2791806958473627e-05,
      "loss": 0.5921,
      "step": 19900
    },
    {
      "epoch": 2.766964100435775,
      "grad_norm": 5.546678066253662,
      "learning_rate": 2.2651515151515152e-05,
      "loss": 0.5976,
      "step": 20000
    },
    {
      "epoch": 2.780798229231514,
      "grad_norm": 5.8153395652771,
      "learning_rate": 2.2511223344556677e-05,
      "loss": 0.5733,
      "step": 20100
    },
    {
      "epoch": 2.794632358027253,
      "grad_norm": 5.287065505981445,
      "learning_rate": 2.2370931537598206e-05,
      "loss": 0.5552,
      "step": 20200
    },
    {
      "epoch": 2.8084664868229923,
      "grad_norm": 5.311277389526367,
      "learning_rate": 2.223063973063973e-05,
      "loss": 0.5661,
      "step": 20300
    },
    {
      "epoch": 2.8223006156187314,
      "grad_norm": 5.615857124328613,
      "learning_rate": 2.209034792368126e-05,
      "loss": 0.5591,
      "step": 20400
    },
    {
      "epoch": 2.8361347444144704,
      "grad_norm": 5.551521301269531,
      "learning_rate": 2.1950056116722785e-05,
      "loss": 0.5707,
      "step": 20500
    },
    {
      "epoch": 2.8499688732102095,
      "grad_norm": 5.693943023681641,
      "learning_rate": 2.180976430976431e-05,
      "loss": 0.5492,
      "step": 20600
    },
    {
      "epoch": 2.8638030020059486,
      "grad_norm": 5.292473316192627,
      "learning_rate": 2.166947250280584e-05,
      "loss": 0.548,
      "step": 20700
    },
    {
      "epoch": 2.8776371308016877,
      "grad_norm": 5.465086936950684,
      "learning_rate": 2.1529180695847364e-05,
      "loss": 0.5529,
      "step": 20800
    },
    {
      "epoch": 2.891471259597427,
      "grad_norm": 5.968790531158447,
      "learning_rate": 2.1390291806958473e-05,
      "loss": 0.5264,
      "step": 20900
    },
    {
      "epoch": 2.905305388393166,
      "grad_norm": 5.342103481292725,
      "learning_rate": 2.125e-05,
      "loss": 0.5344,
      "step": 21000
    },
    {
      "epoch": 2.919139517188905,
      "grad_norm": 5.333170413970947,
      "learning_rate": 2.1109708193041527e-05,
      "loss": 0.5267,
      "step": 21100
    },
    {
      "epoch": 2.932973645984644,
      "grad_norm": 6.284395694732666,
      "learning_rate": 2.0969416386083055e-05,
      "loss": 0.5291,
      "step": 21200
    },
    {
      "epoch": 2.946807774780383,
      "grad_norm": 5.256504058837891,
      "learning_rate": 2.082912457912458e-05,
      "loss": 0.5331,
      "step": 21300
    },
    {
      "epoch": 2.9606419035761222,
      "grad_norm": 5.651012897491455,
      "learning_rate": 2.0688832772166106e-05,
      "loss": 0.5342,
      "step": 21400
    },
    {
      "epoch": 2.9744760323718613,
      "grad_norm": 5.404751777648926,
      "learning_rate": 2.0548540965207634e-05,
      "loss": 0.5143,
      "step": 21500
    },
    {
      "epoch": 2.9883101611676004,
      "grad_norm": 6.161787986755371,
      "learning_rate": 2.040824915824916e-05,
      "loss": 0.5099,
      "step": 21600
    },
    {
      "epoch": 3.0020751193193607,
      "grad_norm": 5.312614917755127,
      "learning_rate": 2.0269360269360272e-05,
      "loss": 0.5038,
      "step": 21700
    },
    {
      "epoch": 3.0159092481150998,
      "grad_norm": 5.548758029937744,
      "learning_rate": 2.0129068462401797e-05,
      "loss": 0.4477,
      "step": 21800
    },
    {
      "epoch": 3.029743376910839,
      "grad_norm": 5.460125923156738,
      "learning_rate": 1.9988776655443322e-05,
      "loss": 0.4515,
      "step": 21900
    },
    {
      "epoch": 3.043577505706578,
      "grad_norm": 5.762377738952637,
      "learning_rate": 1.984848484848485e-05,
      "loss": 0.4418,
      "step": 22000
    },
    {
      "epoch": 3.057411634502317,
      "grad_norm": 5.466300010681152,
      "learning_rate": 1.9708193041526376e-05,
      "loss": 0.4487,
      "step": 22100
    },
    {
      "epoch": 3.071245763298056,
      "grad_norm": 6.162716865539551,
      "learning_rate": 1.95679012345679e-05,
      "loss": 0.4449,
      "step": 22200
    },
    {
      "epoch": 3.085079892093795,
      "grad_norm": 5.448004245758057,
      "learning_rate": 1.9427609427609426e-05,
      "loss": 0.4501,
      "step": 22300
    },
    {
      "epoch": 3.0989140208895343,
      "grad_norm": 6.101217746734619,
      "learning_rate": 1.9287317620650955e-05,
      "loss": 0.4419,
      "step": 22400
    },
    {
      "epoch": 3.1127481496852734,
      "grad_norm": 5.236502647399902,
      "learning_rate": 1.9147025813692483e-05,
      "loss": 0.4406,
      "step": 22500
    },
    {
      "epoch": 3.1265822784810124,
      "grad_norm": 5.043941020965576,
      "learning_rate": 1.900673400673401e-05,
      "loss": 0.4328,
      "step": 22600
    },
    {
      "epoch": 3.140416407276752,
      "grad_norm": 6.055691719055176,
      "learning_rate": 1.8866442199775534e-05,
      "loss": 0.4272,
      "step": 22700
    },
    {
      "epoch": 3.1542505360724906,
      "grad_norm": 5.016207695007324,
      "learning_rate": 1.872615039281706e-05,
      "loss": 0.4351,
      "step": 22800
    },
    {
      "epoch": 3.16808466486823,
      "grad_norm": 5.15708589553833,
      "learning_rate": 1.8585858585858588e-05,
      "loss": 0.4232,
      "step": 22900
    },
    {
      "epoch": 3.181918793663969,
      "grad_norm": 5.6615376472473145,
      "learning_rate": 1.8445566778900113e-05,
      "loss": 0.4278,
      "step": 23000
    },
    {
      "epoch": 3.1957529224597083,
      "grad_norm": 5.4727888107299805,
      "learning_rate": 1.8305274971941638e-05,
      "loss": 0.4253,
      "step": 23100
    },
    {
      "epoch": 3.209587051255447,
      "grad_norm": 5.1737799644470215,
      "learning_rate": 1.8164983164983166e-05,
      "loss": 0.4239,
      "step": 23200
    },
    {
      "epoch": 3.2234211800511865,
      "grad_norm": 5.196803569793701,
      "learning_rate": 1.802469135802469e-05,
      "loss": 0.4122,
      "step": 23300
    },
    {
      "epoch": 3.237255308846925,
      "grad_norm": 6.226336479187012,
      "learning_rate": 1.788439955106622e-05,
      "loss": 0.4166,
      "step": 23400
    },
    {
      "epoch": 3.2510894376426647,
      "grad_norm": 5.380354881286621,
      "learning_rate": 1.7744107744107745e-05,
      "loss": 0.4009,
      "step": 23500
    },
    {
      "epoch": 3.2649235664384033,
      "grad_norm": 5.801249027252197,
      "learning_rate": 1.760381593714927e-05,
      "loss": 0.4143,
      "step": 23600
    },
    {
      "epoch": 3.278757695234143,
      "grad_norm": 5.661643981933594,
      "learning_rate": 1.74635241301908e-05,
      "loss": 0.4076,
      "step": 23700
    },
    {
      "epoch": 3.2925918240298815,
      "grad_norm": 5.240429401397705,
      "learning_rate": 1.7323232323232324e-05,
      "loss": 0.4061,
      "step": 23800
    },
    {
      "epoch": 3.306425952825621,
      "grad_norm": 5.262416362762451,
      "learning_rate": 1.7182940516273853e-05,
      "loss": 0.3877,
      "step": 23900
    },
    {
      "epoch": 3.3202600816213597,
      "grad_norm": 5.359720230102539,
      "learning_rate": 1.7042648709315375e-05,
      "loss": 0.402,
      "step": 24000
    },
    {
      "epoch": 3.334094210417099,
      "grad_norm": 5.1121602058410645,
      "learning_rate": 1.6902356902356903e-05,
      "loss": 0.3979,
      "step": 24100
    },
    {
      "epoch": 3.347928339212838,
      "grad_norm": 5.669027328491211,
      "learning_rate": 1.676206509539843e-05,
      "loss": 0.3986,
      "step": 24200
    },
    {
      "epoch": 3.3617624680085774,
      "grad_norm": 5.228442668914795,
      "learning_rate": 1.6621773288439957e-05,
      "loss": 0.3985,
      "step": 24300
    },
    {
      "epoch": 3.375596596804316,
      "grad_norm": 5.371610164642334,
      "learning_rate": 1.6481481481481482e-05,
      "loss": 0.3859,
      "step": 24400
    },
    {
      "epoch": 3.3894307256000555,
      "grad_norm": 5.932854652404785,
      "learning_rate": 1.6341189674523007e-05,
      "loss": 0.3794,
      "step": 24500
    },
    {
      "epoch": 3.403264854395794,
      "grad_norm": 5.792937278747559,
      "learning_rate": 1.6200897867564536e-05,
      "loss": 0.3818,
      "step": 24600
    },
    {
      "epoch": 3.4170989831915337,
      "grad_norm": 5.404122352600098,
      "learning_rate": 1.606060606060606e-05,
      "loss": 0.379,
      "step": 24700
    },
    {
      "epoch": 3.4309331119872724,
      "grad_norm": 5.394117832183838,
      "learning_rate": 1.592031425364759e-05,
      "loss": 0.387,
      "step": 24800
    },
    {
      "epoch": 3.444767240783012,
      "grad_norm": 5.353161334991455,
      "learning_rate": 1.5780022446689115e-05,
      "loss": 0.37,
      "step": 24900
    },
    {
      "epoch": 3.4586013695787505,
      "grad_norm": 5.296586036682129,
      "learning_rate": 1.563973063973064e-05,
      "loss": 0.3754,
      "step": 25000
    },
    {
      "epoch": 3.47243549837449,
      "grad_norm": 4.977446556091309,
      "learning_rate": 1.549943883277217e-05,
      "loss": 0.3647,
      "step": 25100
    },
    {
      "epoch": 3.486269627170229,
      "grad_norm": 5.470046043395996,
      "learning_rate": 1.5359147025813694e-05,
      "loss": 0.3716,
      "step": 25200
    },
    {
      "epoch": 3.5001037559659682,
      "grad_norm": 6.002427101135254,
      "learning_rate": 1.5218855218855219e-05,
      "loss": 0.3551,
      "step": 25300
    },
    {
      "epoch": 3.513937884761707,
      "grad_norm": 5.322403430938721,
      "learning_rate": 1.5078563411896746e-05,
      "loss": 0.3555,
      "step": 25400
    },
    {
      "epoch": 3.5277720135574464,
      "grad_norm": 5.306790351867676,
      "learning_rate": 1.4938271604938272e-05,
      "loss": 0.3538,
      "step": 25500
    },
    {
      "epoch": 3.541606142353185,
      "grad_norm": 5.273946762084961,
      "learning_rate": 1.47979797979798e-05,
      "loss": 0.3552,
      "step": 25600
    },
    {
      "epoch": 3.5554402711489246,
      "grad_norm": 4.748394012451172,
      "learning_rate": 1.4659090909090909e-05,
      "loss": 0.3547,
      "step": 25700
    },
    {
      "epoch": 3.5692743999446632,
      "grad_norm": 4.937527179718018,
      "learning_rate": 1.4518799102132435e-05,
      "loss": 0.3549,
      "step": 25800
    },
    {
      "epoch": 3.5831085287404028,
      "grad_norm": 4.566598415374756,
      "learning_rate": 1.4378507295173962e-05,
      "loss": 0.3487,
      "step": 25900
    },
    {
      "epoch": 3.5969426575361414,
      "grad_norm": 4.79517126083374,
      "learning_rate": 1.423821548821549e-05,
      "loss": 0.3519,
      "step": 26000
    },
    {
      "epoch": 3.610776786331881,
      "grad_norm": 4.924867153167725,
      "learning_rate": 1.4097923681257016e-05,
      "loss": 0.3347,
      "step": 26100
    },
    {
      "epoch": 3.6246109151276196,
      "grad_norm": 5.8591628074646,
      "learning_rate": 1.3957631874298541e-05,
      "loss": 0.3458,
      "step": 26200
    },
    {
      "epoch": 3.638445043923359,
      "grad_norm": 5.182126045227051,
      "learning_rate": 1.3817340067340068e-05,
      "loss": 0.3404,
      "step": 26300
    },
    {
      "epoch": 3.6522791727190977,
      "grad_norm": 5.266571998596191,
      "learning_rate": 1.3677048260381595e-05,
      "loss": 0.3377,
      "step": 26400
    },
    {
      "epoch": 3.6661133015148373,
      "grad_norm": 5.157329082489014,
      "learning_rate": 1.3536756453423122e-05,
      "loss": 0.3288,
      "step": 26500
    },
    {
      "epoch": 3.679947430310576,
      "grad_norm": 5.273317813873291,
      "learning_rate": 1.3396464646464645e-05,
      "loss": 0.3412,
      "step": 26600
    },
    {
      "epoch": 3.6937815591063154,
      "grad_norm": 5.348573684692383,
      "learning_rate": 1.3256172839506172e-05,
      "loss": 0.3283,
      "step": 26700
    },
    {
      "epoch": 3.707615687902054,
      "grad_norm": 4.796318054199219,
      "learning_rate": 1.31158810325477e-05,
      "loss": 0.3383,
      "step": 26800
    },
    {
      "epoch": 3.7214498166977936,
      "grad_norm": 4.880799293518066,
      "learning_rate": 1.2975589225589228e-05,
      "loss": 0.32,
      "step": 26900
    },
    {
      "epoch": 3.7352839454935327,
      "grad_norm": 4.799187183380127,
      "learning_rate": 1.2835297418630751e-05,
      "loss": 0.3237,
      "step": 27000
    },
    {
      "epoch": 3.749118074289272,
      "grad_norm": 4.722815990447998,
      "learning_rate": 1.2695005611672278e-05,
      "loss": 0.3186,
      "step": 27100
    },
    {
      "epoch": 3.762952203085011,
      "grad_norm": 4.928505897521973,
      "learning_rate": 1.2554713804713805e-05,
      "loss": 0.3317,
      "step": 27200
    },
    {
      "epoch": 3.77678633188075,
      "grad_norm": 5.17380428314209,
      "learning_rate": 1.2414421997755332e-05,
      "loss": 0.3251,
      "step": 27300
    },
    {
      "epoch": 3.790620460676489,
      "grad_norm": 5.543056011199951,
      "learning_rate": 1.2274130190796858e-05,
      "loss": 0.3182,
      "step": 27400
    },
    {
      "epoch": 3.804454589472228,
      "grad_norm": 4.953801155090332,
      "learning_rate": 1.2133838383838385e-05,
      "loss": 0.2989,
      "step": 27500
    },
    {
      "epoch": 3.8182887182679672,
      "grad_norm": 5.559802532196045,
      "learning_rate": 1.199354657687991e-05,
      "loss": 0.3204,
      "step": 27600
    },
    {
      "epoch": 3.8321228470637063,
      "grad_norm": 4.847816467285156,
      "learning_rate": 1.1854657687991021e-05,
      "loss": 0.3065,
      "step": 27700
    },
    {
      "epoch": 3.8459569758594454,
      "grad_norm": 4.646571159362793,
      "learning_rate": 1.1714365881032548e-05,
      "loss": 0.3046,
      "step": 27800
    },
    {
      "epoch": 3.8597911046551845,
      "grad_norm": 5.843827724456787,
      "learning_rate": 1.1574074074074075e-05,
      "loss": 0.2955,
      "step": 27900
    },
    {
      "epoch": 3.8736252334509236,
      "grad_norm": 5.359726428985596,
      "learning_rate": 1.1433782267115602e-05,
      "loss": 0.2915,
      "step": 28000
    },
    {
      "epoch": 3.8874593622466627,
      "grad_norm": 4.814517498016357,
      "learning_rate": 1.1293490460157127e-05,
      "loss": 0.3048,
      "step": 28100
    },
    {
      "epoch": 3.9012934910424018,
      "grad_norm": 5.006430149078369,
      "learning_rate": 1.1153198653198654e-05,
      "loss": 0.303,
      "step": 28200
    },
    {
      "epoch": 3.915127619838141,
      "grad_norm": 3.8539602756500244,
      "learning_rate": 1.101290684624018e-05,
      "loss": 0.2919,
      "step": 28300
    },
    {
      "epoch": 3.92896174863388,
      "grad_norm": 5.181006908416748,
      "learning_rate": 1.0872615039281706e-05,
      "loss": 0.2879,
      "step": 28400
    },
    {
      "epoch": 3.942795877429619,
      "grad_norm": 4.091270923614502,
      "learning_rate": 1.0732323232323233e-05,
      "loss": 0.2883,
      "step": 28500
    },
    {
      "epoch": 3.956630006225358,
      "grad_norm": 4.918316841125488,
      "learning_rate": 1.059203142536476e-05,
      "loss": 0.2873,
      "step": 28600
    },
    {
      "epoch": 3.970464135021097,
      "grad_norm": 5.109428882598877,
      "learning_rate": 1.0451739618406285e-05,
      "loss": 0.2943,
      "step": 28700
    },
    {
      "epoch": 3.9842982638168363,
      "grad_norm": 4.843273639678955,
      "learning_rate": 1.0311447811447812e-05,
      "loss": 0.2812,
      "step": 28800
    },
    {
      "epoch": 3.9981323926125754,
      "grad_norm": 5.013469696044922,
      "learning_rate": 1.0171156004489337e-05,
      "loss": 0.2773,
      "step": 28900
    },
    {
      "epoch": 4.011897350764335,
      "grad_norm": 4.730082035064697,
      "learning_rate": 1.0030864197530866e-05,
      "loss": 0.2536,
      "step": 29000
    },
    {
      "epoch": 4.025731479560075,
      "grad_norm": 4.679319858551025,
      "learning_rate": 9.89057239057239e-06,
      "loss": 0.2511,
      "step": 29100
    },
    {
      "epoch": 4.039565608355813,
      "grad_norm": 4.233829975128174,
      "learning_rate": 9.750280583613918e-06,
      "loss": 0.2593,
      "step": 29200
    },
    {
      "epoch": 4.053399737151553,
      "grad_norm": 5.13645601272583,
      "learning_rate": 9.609988776655445e-06,
      "loss": 0.252,
      "step": 29300
    },
    {
      "epoch": 4.067233865947292,
      "grad_norm": 5.442267894744873,
      "learning_rate": 9.46969696969697e-06,
      "loss": 0.249,
      "step": 29400
    },
    {
      "epoch": 4.081067994743031,
      "grad_norm": 5.39005708694458,
      "learning_rate": 9.329405162738497e-06,
      "loss": 0.2421,
      "step": 29500
    },
    {
      "epoch": 4.09490212353877,
      "grad_norm": 5.404372215270996,
      "learning_rate": 9.189113355780023e-06,
      "loss": 0.2512,
      "step": 29600
    },
    {
      "epoch": 4.108736252334509,
      "grad_norm": 5.179969787597656,
      "learning_rate": 9.050224466891134e-06,
      "loss": 0.2421,
      "step": 29700
    },
    {
      "epoch": 4.122570381130249,
      "grad_norm": 4.9197893142700195,
      "learning_rate": 8.90993265993266e-06,
      "loss": 0.2414,
      "step": 29800
    },
    {
      "epoch": 4.136404509925987,
      "grad_norm": 4.7308573722839355,
      "learning_rate": 8.769640852974186e-06,
      "loss": 0.2375,
      "step": 29900
    },
    {
      "epoch": 4.150238638721726,
      "grad_norm": 3.9322729110717773,
      "learning_rate": 8.629349046015713e-06,
      "loss": 0.2327,
      "step": 30000
    },
    {
      "epoch": 4.164072767517466,
      "grad_norm": 4.496487617492676,
      "learning_rate": 8.48905723905724e-06,
      "loss": 0.2295,
      "step": 30100
    },
    {
      "epoch": 4.177906896313205,
      "grad_norm": 4.8822126388549805,
      "learning_rate": 8.348765432098765e-06,
      "loss": 0.2415,
      "step": 30200
    },
    {
      "epoch": 4.191741025108944,
      "grad_norm": 4.329873085021973,
      "learning_rate": 8.208473625140292e-06,
      "loss": 0.226,
      "step": 30300
    },
    {
      "epoch": 4.205575153904682,
      "grad_norm": 4.701030254364014,
      "learning_rate": 8.068181818181819e-06,
      "loss": 0.2434,
      "step": 30400
    },
    {
      "epoch": 4.219409282700422,
      "grad_norm": 4.559396266937256,
      "learning_rate": 7.927890011223344e-06,
      "loss": 0.2348,
      "step": 30500
    },
    {
      "epoch": 4.2332434114961615,
      "grad_norm": 4.774644374847412,
      "learning_rate": 7.787598204264871e-06,
      "loss": 0.2317,
      "step": 30600
    },
    {
      "epoch": 4.2470775402919,
      "grad_norm": 4.61735725402832,
      "learning_rate": 7.647306397306398e-06,
      "loss": 0.2338,
      "step": 30700
    },
    {
      "epoch": 4.260911669087639,
      "grad_norm": 4.582118988037109,
      "learning_rate": 7.507014590347925e-06,
      "loss": 0.2263,
      "step": 30800
    },
    {
      "epoch": 4.274745797883378,
      "grad_norm": 4.790769577026367,
      "learning_rate": 7.36672278338945e-06,
      "loss": 0.2313,
      "step": 30900
    },
    {
      "epoch": 4.288579926679118,
      "grad_norm": 4.925541400909424,
      "learning_rate": 7.226430976430977e-06,
      "loss": 0.23,
      "step": 31000
    },
    {
      "epoch": 4.3024140554748564,
      "grad_norm": 4.700111389160156,
      "learning_rate": 7.086139169472503e-06,
      "loss": 0.2283,
      "step": 31100
    },
    {
      "epoch": 4.316248184270595,
      "grad_norm": 4.745042324066162,
      "learning_rate": 6.94584736251403e-06,
      "loss": 0.2199,
      "step": 31200
    },
    {
      "epoch": 4.330082313066335,
      "grad_norm": 4.37134313583374,
      "learning_rate": 6.805555555555556e-06,
      "loss": 0.2329,
      "step": 31300
    },
    {
      "epoch": 4.343916441862074,
      "grad_norm": 5.201396465301514,
      "learning_rate": 6.665263748597083e-06,
      "loss": 0.2164,
      "step": 31400
    },
    {
      "epoch": 4.357750570657813,
      "grad_norm": 4.285497188568115,
      "learning_rate": 6.5249719416386094e-06,
      "loss": 0.212,
      "step": 31500
    },
    {
      "epoch": 4.371584699453552,
      "grad_norm": 4.826619625091553,
      "learning_rate": 6.384680134680135e-06,
      "loss": 0.2176,
      "step": 31600
    },
    {
      "epoch": 4.385418828249291,
      "grad_norm": 4.186726093292236,
      "learning_rate": 6.2457912457912455e-06,
      "loss": 0.2211,
      "step": 31700
    },
    {
      "epoch": 4.3992529570450305,
      "grad_norm": 4.360568523406982,
      "learning_rate": 6.105499438832772e-06,
      "loss": 0.2149,
      "step": 31800
    },
    {
      "epoch": 4.413087085840769,
      "grad_norm": 4.458884239196777,
      "learning_rate": 5.965207631874299e-06,
      "loss": 0.2082,
      "step": 31900
    },
    {
      "epoch": 4.426921214636509,
      "grad_norm": 4.287570953369141,
      "learning_rate": 5.824915824915825e-06,
      "loss": 0.2095,
      "step": 32000
    },
    {
      "epoch": 4.440755343432247,
      "grad_norm": 5.114534378051758,
      "learning_rate": 5.684624017957352e-06,
      "loss": 0.2165,
      "step": 32100
    },
    {
      "epoch": 4.454589472227987,
      "grad_norm": 4.3833417892456055,
      "learning_rate": 5.544332210998878e-06,
      "loss": 0.2087,
      "step": 32200
    },
    {
      "epoch": 4.4684236010237255,
      "grad_norm": 3.977339506149292,
      "learning_rate": 5.404040404040404e-06,
      "loss": 0.2197,
      "step": 32300
    },
    {
      "epoch": 4.482257729819465,
      "grad_norm": 4.350746154785156,
      "learning_rate": 5.263748597081931e-06,
      "loss": 0.2157,
      "step": 32400
    },
    {
      "epoch": 4.496091858615204,
      "grad_norm": 4.419335842132568,
      "learning_rate": 5.123456790123457e-06,
      "loss": 0.2155,
      "step": 32500
    },
    {
      "epoch": 4.509925987410943,
      "grad_norm": 4.140323638916016,
      "learning_rate": 4.983164983164983e-06,
      "loss": 0.208,
      "step": 32600
    },
    {
      "epoch": 4.523760116206682,
      "grad_norm": 4.3487772941589355,
      "learning_rate": 4.84287317620651e-06,
      "loss": 0.2118,
      "step": 32700
    },
    {
      "epoch": 4.5375942450024205,
      "grad_norm": 4.643756866455078,
      "learning_rate": 4.702581369248036e-06,
      "loss": 0.2046,
      "step": 32800
    },
    {
      "epoch": 4.55142837379816,
      "grad_norm": 5.309207916259766,
      "learning_rate": 4.562289562289562e-06,
      "loss": 0.2016,
      "step": 32900
    },
    {
      "epoch": 4.5652625025938995,
      "grad_norm": 3.819897174835205,
      "learning_rate": 4.421997755331089e-06,
      "loss": 0.1967,
      "step": 33000
    },
    {
      "epoch": 4.579096631389638,
      "grad_norm": 4.436297416687012,
      "learning_rate": 4.281705948372615e-06,
      "loss": 0.1996,
      "step": 33100
    },
    {
      "epoch": 4.592930760185378,
      "grad_norm": 3.826676845550537,
      "learning_rate": 4.141414141414142e-06,
      "loss": 0.2052,
      "step": 33200
    },
    {
      "epoch": 4.606764888981116,
      "grad_norm": 4.454731464385986,
      "learning_rate": 4.001122334455669e-06,
      "loss": 0.1975,
      "step": 33300
    },
    {
      "epoch": 4.620599017776856,
      "grad_norm": 4.535665035247803,
      "learning_rate": 3.860830527497195e-06,
      "loss": 0.1956,
      "step": 33400
    },
    {
      "epoch": 4.6344331465725945,
      "grad_norm": 4.294256210327148,
      "learning_rate": 3.720538720538721e-06,
      "loss": 0.1967,
      "step": 33500
    },
    {
      "epoch": 4.648267275368334,
      "grad_norm": 4.16081428527832,
      "learning_rate": 3.580246913580247e-06,
      "loss": 0.1885,
      "step": 33600
    },
    {
      "epoch": 4.662101404164073,
      "grad_norm": 4.028237819671631,
      "learning_rate": 3.441358024691358e-06,
      "loss": 0.1948,
      "step": 33700
    },
    {
      "epoch": 4.675935532959812,
      "grad_norm": 4.388003826141357,
      "learning_rate": 3.3010662177328845e-06,
      "loss": 0.1914,
      "step": 33800
    },
    {
      "epoch": 4.689769661755551,
      "grad_norm": 4.006651878356934,
      "learning_rate": 3.160774410774411e-06,
      "loss": 0.2013,
      "step": 33900
    },
    {
      "epoch": 4.70360379055129,
      "grad_norm": 4.181715488433838,
      "learning_rate": 3.0204826038159373e-06,
      "loss": 0.19,
      "step": 34000
    },
    {
      "epoch": 4.717437919347029,
      "grad_norm": 4.096745491027832,
      "learning_rate": 2.8801907968574638e-06,
      "loss": 0.19,
      "step": 34100
    },
    {
      "epoch": 4.731272048142769,
      "grad_norm": 4.313976287841797,
      "learning_rate": 2.73989898989899e-06,
      "loss": 0.1897,
      "step": 34200
    },
    {
      "epoch": 4.745106176938507,
      "grad_norm": 3.683558702468872,
      "learning_rate": 2.5996071829405162e-06,
      "loss": 0.1946,
      "step": 34300
    },
    {
      "epoch": 4.758940305734247,
      "grad_norm": 3.9844868183135986,
      "learning_rate": 2.4593153759820427e-06,
      "loss": 0.1924,
      "step": 34400
    },
    {
      "epoch": 4.772774434529985,
      "grad_norm": 3.6298646926879883,
      "learning_rate": 2.319023569023569e-06,
      "loss": 0.1907,
      "step": 34500
    },
    {
      "epoch": 4.786608563325725,
      "grad_norm": 4.540492057800293,
      "learning_rate": 2.1787317620650956e-06,
      "loss": 0.1809,
      "step": 34600
    },
    {
      "epoch": 4.800442692121464,
      "grad_norm": 3.8660011291503906,
      "learning_rate": 2.038439955106622e-06,
      "loss": 0.1842,
      "step": 34700
    },
    {
      "epoch": 4.814276820917203,
      "grad_norm": 3.8338217735290527,
      "learning_rate": 1.8981481481481482e-06,
      "loss": 0.1895,
      "step": 34800
    },
    {
      "epoch": 4.828110949712942,
      "grad_norm": 4.8874640464782715,
      "learning_rate": 1.7578563411896745e-06,
      "loss": 0.1844,
      "step": 34900
    },
    {
      "epoch": 4.841945078508681,
      "grad_norm": 4.399554252624512,
      "learning_rate": 1.617564534231201e-06,
      "loss": 0.1856,
      "step": 35000
    },
    {
      "epoch": 4.85577920730442,
      "grad_norm": 4.237926483154297,
      "learning_rate": 1.4772727272727273e-06,
      "loss": 0.1887,
      "step": 35100
    },
    {
      "epoch": 4.8696133361001595,
      "grad_norm": 4.4308390617370605,
      "learning_rate": 1.3369809203142536e-06,
      "loss": 0.1848,
      "step": 35200
    },
    {
      "epoch": 4.883447464895898,
      "grad_norm": 3.919494152069092,
      "learning_rate": 1.1966891133557802e-06,
      "loss": 0.1924,
      "step": 35300
    },
    {
      "epoch": 4.897281593691638,
      "grad_norm": 4.126457691192627,
      "learning_rate": 1.0563973063973064e-06,
      "loss": 0.1827,
      "step": 35400
    },
    {
      "epoch": 4.911115722487376,
      "grad_norm": 4.833098888397217,
      "learning_rate": 9.161054994388328e-07,
      "loss": 0.1827,
      "step": 35500
    },
    {
      "epoch": 4.924949851283116,
      "grad_norm": 4.075370788574219,
      "learning_rate": 7.758136924803592e-07,
      "loss": 0.1834,
      "step": 35600
    },
    {
      "epoch": 4.938783980078854,
      "grad_norm": 4.174455165863037,
      "learning_rate": 6.369248035914703e-07,
      "loss": 0.1854,
      "step": 35700
    },
    {
      "epoch": 4.952618108874594,
      "grad_norm": 4.291958808898926,
      "learning_rate": 4.966329966329966e-07,
      "loss": 0.1805,
      "step": 35800
    },
    {
      "epoch": 4.966452237670333,
      "grad_norm": 3.712390899658203,
      "learning_rate": 3.5634118967452304e-07,
      "loss": 0.1853,
      "step": 35900
    },
    {
      "epoch": 4.980286366466072,
      "grad_norm": 4.277632236480713,
      "learning_rate": 2.1604938271604937e-07,
      "loss": 0.181,
      "step": 36000
    },
    {
      "epoch": 4.994120495261811,
      "grad_norm": 4.537714958190918,
      "learning_rate": 7.575757575757576e-08,
      "loss": 0.1818,
      "step": 36100
    }
  ],
  "logging_steps": 100,
  "max_steps": 36140,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.90171041034112e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
